21,Mena Trott,Meet the founder of the blog revolution,"0:11


Over the past couple of days,
as I've been preparing for my speech,
I've become more and more nervous
about what I'm going to say
and about being on the same stage
as all these fascinating people.
Being on the same stage as Al Gore,
who was the first person I ever voted for.
And 




 0:27


(Laughter)




 0:31


So I was getting pretty nervous
and, you know, I didn't know
that Chris sits on the stage,
and that's more nerve-racking.
But then I started thinking
about my family.
I started thinking about my father
and my grandfather
and my great-grandfather,
and I realized
that I had all of these Teds
going through my bloodstream 




 0:50


(Laughter)




 0:51


that I had to consider this ""my element.""




 0:55


So, who am I?
Chris kind of mentioned I started
a company with my husband.
We have about 125 people internationally.
If you looked in the book,
you saw this ...




 1:06


(Laughter)




 1:07


which I really was appalled by.




 1:11


(Laughter)




 1:13


And because I wanted
to impress you all with slides,
since I saw the great presentations
yesterday with graphs,
I made a graph that moves,
and I talk about the makeup of me.




 1:25


(Laughter)




 1:28


So, besides this freakish thing,
this is my science slide.
This is math, and this is science,
this is genetics.
This is my grandmother,
and this is where I get this mouth.




 1:37


(Laughter)




 1:39


So 
I'm a blogger, which, probably,
to a lot of you, means different things.
You may have heard
about the Kryptonite lock brouhaha,
where a blogger talked about how you hack
or break into a Kryptonite
lock using a ballpoint pen,
and it spread all over.
Kryptonite had to adjust the lock,
and they had to address it
to avoid too many customer concerns.
You may have heard about Rathergate,
which was basically the result of bloggers
realizing that the ""th"" in 111
is not typeset on an old typewriter;
it's on Word.
Bloggers exposed this,
or they worked hard to expose this.
You know, blogs are scary.
This is what you see.
I see this, and I'm sure scared 
I swear on stage  shitless about blogs,
because this is not
something that's friendly.
But there are blogs that are changing
the way we read news
and consume media,
and these are great examples.
These people are reaching thousands,
if not millions, of readers,
and that's incredibly important.
During the hurricane,
you had MSNBC posting
about the hurricane on their blog,
updating it frequently.
This was possible because of the easy
nature of blogging tools.




 2:52


You have my friend,
who has a blog on PVRs,
personal recorders.
He makes enough money just by running ads,
to support his family up in Oregon.
That's all he does now,
and this is something
that blogs have made possible.
And then you have something
like this, which is Interplast.
It's a wonderful organization
of people and doctors
who go to developing nations
to offer plastic surgery
to those who need it.
Children with cleft palates get it,
and they document their story.
This is wonderful.
I am not that caring.




 3:25


(Laughter)




 3:27


I talk about myself.
That's what I am. I'm a blogger.
I have always decided that I was going
to be an expert on one thing,
and I am an expert on this person,
and so I write about it.
So, the short story about my blog:
it started in 2001, I was 23.
I wasn't happy with my job,
because I was a designer,
but I wasn't being really stimulated.
I was an English major in college.
I didn't have any use for it,
but I missed writing.
So, I started to write a blog
and I started to create things
like these little stories.
This was an illustration about my camp
experience when I was 11 years old,
and how I went to a YMCA camp,
Christian camp,
and basically by the end,
I had made my friends hate me so much
that I hid in a bunk,
They couldn't find me,
they sent a search party,
and I overheard people saying
they wish I had killed myself 
jumped off Bible Peak.




 4:17


You can laugh, this is OK.




 4:19


(Laughter)




 4:20


This is me.
This is what happened to me.
And when I started my blog,
it was really this one goal 
I said, ""I am not going
to be famous to the world,
but I could be famous
to people on the Internet.""
And I set a goal.
I said, ""I'm going to win an award,""
because I had never won
an award in my entire life.
And I said, ""I'm going to win
the South by Southwest Weblog award.""
And I won it  I reached
all of these people,
and I had tens of thousands of people
reading about my life every day.




 4:52


And then I wrote a post about a banjo.
I wrote a post
about wanting to buy a banjo 
a $300 banjo, which is a lot of money.
And I don't play instruments;
I don't know anything about music.
I like music, and I like banjos,
and I think I probably heard
Steve Martin playing,
and I said, ""I could do that.""
And I said to my husband,
""Ben, can I buy a banjo?""
And he's like, ""No.""




 5:16


And my husband 




 5:17


(Laughter)




 5:18


this is my husband, who is very hot 
he won an award for being hot.




 5:23


(Laughter)




 5:25


He told me,
""You cannot buy a banjo.
You're just like your dad,""
who collects instruments.
And I wrote a post
about how I was so mad at him,
he was such a tyrant 
he would not let me buy this banjo.
And those people who know me
understood my joke 
this is Mena, this is how
I make a joke at people.
Because the joke in this
is that this person is not a tyrant,
this person is so loving and so sweet
that he lets me dress him up
and post pictures of him to my blog.




 5:53


(Laughter)




 5:57


And if he knew I was showing
this right now  I put this in today 
he would kill me.




 6:02


But the thing was, my friends read it,
and they're like, ""Oh, that Mena,
she wrote a post
about wanting a stupid thing
and being stupid.""
But I got emails from people that said,
""Oh my God, your husband
is such an asshole.
How much money does
he spend on beer in a year?
You could take that money
and buy your banjo.
Why don't you open a separate account?""
I've been with him since I was 17,
we've never had a separate bank account.
They said, ""Separate your bank account.
Spend your money;
spend his money, that's it.""
And then I got people saying, ""Leave him.""




 6:33


(Laughter)




 6:34


I was like, ""OK, what?
Who are these people?
And why are they reading this?""
And I realized: I don't
want to reach these people.
I don't want to write
for this public audience.
And I started to kill my blog slowly.
I'm like, I don't want
to write this anymore.
Slowly and slowly 




 6:49


And I did tell personal
stories from time to time.
I wrote this one, and I put this
up because of Einstein today.
I'm going to get choked up,
because this is my first pet,
and she passed away two years ago.
And I decided to break from, ""I don't
really write about my public life,""
because I wanted to give her
a little memorial.
But anyways, it's these sorts
of personal stories 
You know, you read the blogs
about politics or about media,
and gossip and all these things.
These are out there,
but it's more of the personal
that interests me,
and this is who I am.




 7:19


You see Norman Rockwell,
and you have art critics say,
""Norman Rockwell is not art.
Norman Rockwell hangs
in living rooms and bathrooms,
and this is not something
to be considered high art.""
And I think this is one
of the most important things
to us as humans.
These things resonate with us,
and, if you think about blogs,
you think of high art blogs,
the history paintings about,
you know, all the biblical stories,
and then you have this.
These are the blogs that interest me:
the people that just tell stories.




 7:49


One story is about this baby,
and his name is Odin.
His father was a blogger.
And he was writing his blog one day,
and his wife gave birth to her baby
at 25 weeks.
And he never expected this.
One day, it was normal;
the next day, it was hell.
And this is a one-pound baby.
So Odin was documented every single day.




 8:13


Pictures were taken every day:
day one, day two ...
You have day nine 
they're talking about his apnea;
day 39  he gets pneumonia.
His baby is so small,
and I've never encountered such a 
just  a disturbing image,
but just so heartfelt.
And you're reading this as it happens,
so on day 55, everybody reads
that he's having failures:
breathing failures and heart failures,
and it's slowing down,
and you don't know what to expect.




 8:42


But then it gets better.
Day 96, he goes home.
And you see this post.
That's not something you're going
to see in a paper or magazine
but this is something this person feels,
and people are excited
about it  28 comments.
That's not a huge amount
of people reading,
but 28 people matter.
And today, he is a healthy baby,
who, if you read his blog 
it's snowdeal.org, his father's blog 
he is taking pictures of him still,
because he is still his son
and he is, I think,
at his age level right now
because he had received such
great treatment from the hospital.




 9:17


So, blogs.
So what? You've probably
heard these things before.
We talked about the WELL,
and about all these sorts of things
throughout our online history.
But I think blogs are basically
just an evolution,
and that's where we are today.
It's this record
of who you are, your persona.
You have your Google search,
where you say, ""What is Mena Trott?""
And then you find these things
and you're happy or unhappy.
But then you also find people's blogs,
and those are the records of people
that are writing daily 
not necessarily about the same topic,
but things that interest them.
And we talk about the world
flattens, being in this panel,
and I am very optimistic 
whenever I think about blogs,
I'm like, ""We've got to reach
all these people.""
Hundreds of millions
and billions of people.
We're getting into China,
we want to be there,
but there are so many people that
won't have the access to write a blog.
But to see something
like the $100 computer is amazing,
because blogging software is simple.
We have a successful company
because of timing,
and because of perseverance,
but it's simple stuff 
it's not rocket science.
And so, that's an amazing
thing to consider.
So 
the life record of a blog is something
that I find incredibly important.




10:30


And we started with a slide of my Teds,
and I had to add this slide,
because I knew the minute
I showed this, my mom 
my mom will see this,
because she does read my blog
and she'll say, ""Why wasn't there
a picture of me?""
This is my mom.
So, I have all the people that I know of.
But this is basically the extent
of the family that I know
in terms of my direct line.
I showed a Norman Rockwell painting
before, and this one, I grew up with,
looking at constantly.
I would spend hours
looking at the connections,
saying, ""Oh, the little kid
up at the top has red hair;
so does that first generation up there.""
And it's just these little things.
This is not science,
but this was enough for me
to be really interested
in how we have evolved
and how we can trace our line.
So that has always influenced me.




11:19


I have this record, this 1910 census,
of another Grabowski 
that's my maiden name 
and there's a Theodore,
because there's always a Theodore.
This is all I have,
a couple of facts about somebody.
I have their date of birth, their age,
what they did in their household,
if they spoke English,
and that's it, that's all I know
of these people.
And it's pretty sad,
because I only go back
five generations, and that's it.
I don't even know what happens
on my mom's side,
because she's from Cuba
and I don't have that many things.
Just doing this,
I spent time in the archives 
that's why my husband's a saint 
I spent time in the Washington archives,
just sitting there,
looking for these things.
Now it's online, but he sat through that.




12:01


And so you have this record and 
This is my great-great-grandmother.
This is the only picture I have.
And to think of what we have the ability
to do with our blogs;
to think about the people
that are on those $100 computers,
talking about who they are,
sharing these personal stories 
this is an amazing thing.




12:20


Another photo that has
greatly influenced me,
or a series of photos, is this project
that's done by an Argentinean
man and his wife.
And he's basically taking a picture
of his family every day
for the past, what is '76?  20 ...
Oh my God, I'm '77  29 years?
Twenty-nine years.




12:41


There was a joke, originally,
about my graph that I left out, which is:
You see all this math?
I'm just happy I was able
to add it up to 100,
because that's my skill set.




12:49


(Laughter)




12:51


So you have these people aging,
and now this is them today, or last year.
And that's a powerful thing
to have, to be able to track this.
I wish that I would have
this of my family.
I know that one day my children
will be wondering 
or my grandchildren,
or my great-grandchildren,
if I ever have children 
what I am going to 
who I was.
So I do something
that's very narcissistic 




13:18


I am a blogger 
that is an amazing thing for me,
because it captures
a moment in time every day.
I take a picture of myself 
I've been doing this since last year 
every single day.
And, you know, it's the same picture;
it's basically the same person.




13:33


Only a couple of people read it.
I don't write this for this audience;
I'm showing it now, but I would go
insane if this was really public.
About four people probably read it,
and they tell me, ""You haven't updated.""
I'm probably going to get people
telling me I haven't updated.
But this is amazing, because I can
go back to a day  to April 2005,
and say, what was I doing this day?
I look at it, I know exactly.
It's this visual cue
that is so important to what we do.
I put the bad pictures up too,
because there are bad pictures.




14:02


(Laughter)




14:03


And I remember instantly:
I am in Germany in this 
I had to go for a one-day trip.
I was sick, and I was in a hotel room,
and I wanted not to be there.
And so you see these things,
it's not just always smiling.
Now I've kind of evolved it,
so I have this look.
If you look at my driver's license,
I have the same look,
and it's a pretty disturbing thing,
but it's something
that is really important.




14:27


And the last story
I really want to tell is this story,
because this is probably the one
that means the most to me
in all of what I'm doing.
I'll probably get choked up,
because I tend to when I talk about this.
So, this woman, her name was Emma,
and she was a blogger
on our service, TypePad.
And she was a beta tester,
so she was there right when we opened 
you know, there was 100 people.
And she wrote about her life
dealing with cancer.
She was writing and writing,
and we all started reading it,
because we had
so few blogs on the service,
we could keep track of everyone.




15:00


And she was writing one day,
and then she disappeared for a little bit.
And her sister came on, and she said
that Emma had passed away.
And all of our support staff who had
talked to her were really emotional,
and it was a very hard day at the company.




15:17


And this was one of those
instances where I realized
how much blogging
affects our relationship,
and flattening this sort of world.
That this woman is in England,
and she lives  she lived  a life
where she was talking
about what she was doing.
But the big thing
that really influenced us was,
her sister wrote to me, and she said 
and she wrote on this blog 
that writing her blog during
the last couple of months of her life
was probably the best thing
that had happened to her,
and being able to talk to people
and to share what was going on,
and being able to write
and receive comments.
And that was amazing,
to be able to know
that we had empowered that,
and that blogging was something
that she felt comfortable doing,
and the idea that blogging
doesn't have to be scary,
that we don't always have
to be attack of the blogs,
that we can be people who are open,
and wanting to help and talk to people.
That was an amazing thing.




16:10


And so I printed out and sent
a PDF of her blog to her family,
and they passed it
out at her memorial service,
and even in her obituary,
they mentioned her blog,
because it was such
a big part of her life.
And that's a huge thing.




16:23


So, this is her legacy,
and I think that my call to action
to all of you is:
think about blogs,
think about what they are,
think about what you've thought of them,
and then actually do it,
because it's something
that's really going to change our lives.




16:38


So, thank you.




16:39


(Applause)",2017-06-13 11:01:15.211558
1,Al Gore,Averting the climate crisis,"0:14


Thank you so much, Chris.
And it's truly a great honor
to have the opportunity
to come to this stage twice;
I'm extremely grateful.
I have been blown away by this conference,
and I want to thank all of you
for the many nice comments
about what I had to say the other night.
And I say that sincerely,
partly because (Mock sob)
I need that.




 0:40


(Laughter)




 0:45


Put yourselves in my position.




 0:47


(Laughter)




 0:54


I flew on Air Force Two for eight years.




 0:57


(Laughter)




 0:59


Now I have to take off my shoes
or boots to get on an airplane!




 1:02


(Laughter)




 1:05


(Applause)




 1:11


I'll tell you one quick story
to illustrate what
that's been like for me.




 1:16


(Laughter)




 1:18


It's a true story 
every bit of this is true.




 1:21


Soon after Tipper and I left the 
(Mock sob) White House 




 1:24


(Laughter)




 1:26


we were driving from our home
in Nashville to a little farm we have
50 miles east of Nashville.
Driving ourselves.




 1:36


(Laughter)




 1:39


I know it sounds like
a little thing to you, but 




 1:41


(Laughter)




 1:47


I looked in the rear-view mirror
and all of a sudden it just hit me.
There was no motorcade back there.




 1:57


(Laughter)




 2:00


You've heard of phantom limb pain?




 2:02


(Laughter)




 2:07


This was a rented Ford Taurus.




 2:11


(Laughter)




 2:14


It was dinnertime,
and we started looking for a place to eat.
We were on I-40.
We got to Exit 238, Lebanon, Tennessee.
We got off the exit,
we found a Shoney's restaurant.
Low-cost family restaurant chain,
for those of you who don't know it.
We went in and sat down at the booth,
and the waitress came over,
made a big commotion over Tipper.




 2:39


(Laughter)




 2:41


She took our order, and then went
to the couple in the booth next to us,
and she lowered her voice so much,
I had to really strain to hear
what she was saying.
And she said ""Yes, that's former
Vice President Al Gore
and his wife, Tipper.""
And the man said,
""He's come down a long way, hasn't he?""




 2:58


(Laughter)




 3:03


(Applause)




 3:08


There's been kind
of a series of epiphanies.




 3:11


(Laughter)




 3:12


The very next day,
continuing the totally true story,
I got on a G-V to fly to Africa
to make a speech in Nigeria,
in the city of Lagos,
on the topic of energy.
And I began the speech
by telling them the story
of what had just happened
the day before in Nashville.
And I told it pretty much the same way
I've just shared it with you:
Tipper and I were driving ourselves,
Shoney's, low-cost
family restaurant chain,
what the man said  they laughed.
I gave my speech, then went back
out to the airport to fly back home.
I fell asleep on the plane
until, during the middle
of the night, we landed
on the Azores Islands for refueling.
I woke up, they opened the door,
I went out to get some fresh air,
and I looked, and there was a man
running across the runway.
And he was waving a piece
of paper, and he was yelling,
""Call Washington! Call Washington!""
And I thought to myself,
in the middle of the night,
in the middle of the Atlantic,
what in the world could be
wrong in Washington?
Then I remembered
it could be a bunch of things.




 4:15


(Laughter)




 4:21


But what it turned out to be,
was that my staff was extremely upset
because one of the wire services
in Nigeria had already written a story
about my speech,
and it had already been printed in cities
all across the United States of America.
It was printed in Monterey, I checked.




 4:40


(Laughter)




 4:41


And the story began,
""Former Vice President Al Gore
announced in Nigeria yesterday,"" quote:
'My wife Tipper and I have opened
a low-cost family restaurant'"" 




 4:52


(Laughter)




 4:53


""'named Shoney's,
and we are running it ourselves.'""




 4:57


(Laughter)




 5:00


Before I could get back to U.S. soil,
David Letterman and Jay Leno
had already started in on 
one of them had me
in a big white chef's hat,
Tipper was saying,
""One more burger with fries!""




 5:12


(Laughter)




 5:13


Three days later,
I got a nice, long, handwritten letter
from my friend and partner
and colleague Bill Clinton, saying,
""Congratulations
on the new restaurant, Al!""




 5:23


(Laughter)




 5:30


We like to celebrate
each other's successes in life.




 5:33


(Laughter)




 5:39


I was going to talk
about information ecology.
But I was thinking that,
since I plan to make a lifelong habit
of coming back to TED,
that maybe I could talk
about that another time.




 5:50


(Applause)




 5:52


Chris Anderson: It's a deal!




 5:53


(Applause)




 5:56


Al Gore: I want to focus
on what many of you have said
you would like me to elaborate on:
What can you do about the climate crisis?
I want to start with a couple of 
I'm going to show some new images,
and I'm going to recapitulate
just four or five.
Now, the slide show.
I update the slide show
every time I give it.
I add new images,
because I learn more about it
every time I give it.
It's like beach-combing, you know?
Every time the tide comes in and out,
you find some more shells.
Just in the last two days, we got
the new temperature records in January.
This is just for
the United States of America.
Historical average
for Januarys is 31 degrees;
last month was 39.5 degrees.




 6:49


Now, I know that you wanted some more
bad news about the environment 
I'm kidding.
But these are the recapitulation slides,
and then I'm going to go into new
material about what you can do.
But I wanted to elaborate
on a couple of these.
First of all, this is where
we're projected to go
with the U.S. contribution
to global warming,
under business as usual.
Efficiency in end-use electricity
and end-use of all energy
is the low-hanging fruit.
Efficiency and conservation 
it's not a cost; it's a profit.
The sign is wrong.
It's not negative; it's positive.
These are investments
that pay for themselves.
But they are also very effective
in deflecting our path.




 7:36


Cars and trucks  I talked
about that in the slideshow,
but I want you to put it in perspective.
It's an easy, visible target of concern 
and it should be 
but there is more global warming pollution
that comes from buildings
than from cars and trucks.
Cars and trucks are very significant,
and we have the lowest
standards in the world.
And so we should address that.
But it's part of the puzzle.
Other transportation efficiency
is as important as cars and trucks.
Renewables at the current levels
of technological efficiency
can make this much difference.
And with what Vinod, and John Doerr
and others, many of you here 
there are a lot of people
directly involved in this 
this wedge is going to grow
much more rapidly
than the current projection shows it.
Carbon Capture and Sequestration 
that's what CCS stands for 
is likely to become the killer app
that will enable us
to continue to use fossil fuels
in a way that is safe.
Not quite there yet.
OK. Now, what can you do?




 8:57


Reduce emissions in your home.
Most of these expenditures
are also profitable.
Insulation, better design.
Buy green electricity where you can.
I mentioned automobiles  buy a hybrid.
Use light rail.
Figure out some of the other options
that are much better.
It's important.




 9:21


Be a green consumer.
You have choices with everything you buy,
between things that have a harsh effect,
or a much less harsh effect
on the global climate crisis.
Consider this:
Make a decision to live
a carbon-neutral life.
Those of you who are good at branding,
I'd love to get your advice and help
on how to say this in a way
that connects with the most people.
It is easier than you think.
It really is.
A lot of us in here
have made that decision,
and it is really pretty easy.
It means reduce
your carbon dioxide emissions
with the full range
of choices that you make,
and then purchase or acquire offsets
for the remainder that you have not
completely reduced.
And what it means is elaborated
at climatecrisis.net.




10:26


There is a carbon calculator.
Participant Productions convened 
with my active involvement 
the leading software writers in the world,
on this arcane science
of carbon calculation,
to construct a consumer-friendly
carbon calculator.
You can very precisely calculate
what your CO2 emissions are,
and then you will be given
options to reduce.
And by the time the movie
comes out in May,
this will be updated to 2.0,
and we will have click-through
purchases of offsets.




11:06


Next, consider making
your business carbon-neutral.
Again, some of us have done that,
and it's not as hard as you think.
Integrate climate solutions
into all of your innovations,
whether you are from the technology,
or entertainment, or design
and architecture community.
Invest sustainably.
Majora mentioned this.
Listen, if you have invested money
with managers who you compensate
on the basis of their annual performance,
don't ever again complain
about quarterly report CEO management.
Over time, people do
what you pay them to do.
And if they judge how much
they're going to get paid
on your capital that they've invested,
based on the short-term returns,
you're going to get short-term decisions.
A lot more to be said about that.




12:03


Become a catalyst of change.
Teach others, learn about it,
talk about it.
The movie is a movie version
of the slideshow
I gave two nights ago,
except it's a lot more entertaining.
And it comes out in May.
Many of you here have the opportunity
to ensure that a lot of people see it.
Consider sending somebody to Nashville.
Pick well.
And I am personally going to train people
to give this slideshow 
re-purposed, with some
of the personal stories obviously replaced
with a generic approach,
and it's not just the slides,
it's what they mean.
And it's how they link together.
And so I'm going to be conducting
a course this summer
for a group of people that are
nominated by different folks
to come and then give it en masse,
in communities all across the country,
and we're going to update the slideshow
for all of them every single week,
to keep it right on the cutting edge.
Working with Larry Lessig, it will be,
somewhere in that process,
posted with tools
and limited-use copyrights,
so that young people can remix it
and do it in their own way.




13:23


(Applause)




13:26


Where did anybody get the idea
that you ought to stay
arm's length from politics?
It doesn't mean
that if you're a Republican,
that I'm trying to convince you
to be a Democrat.
We need Republicans as well.
This used to be a bipartisan issue,
and I know that
in this group it really is.
Become politically active.
Make our democracy work the way
it's supposed to work.
Support the idea of capping
carbon dioxide emissions 
global warming pollution 
and trading it.
Here's why: as long as the United States
is out of the world system,
it's not a closed system.
Once it becomes a closed system,
with U.S. participation,
then everybody
who's on a board of directors 
how many people here
serve on the board of directors
of a corporation?
Once it's a closed system,
you will have legal liability
if you do not urge your CEO
to get the maximum income from reducing
and trading the carbon emissions
that can be avoided.
The market will work
to solve this problem 
if we can accomplish this.
Help with the mass persuasion campaign
that will start this spring.
We have to change the minds
of the American people.
Because presently, the politicians
do not have permission
to do what needs to be done.




14:44


And in our modern country, the role
of logic and reason no longer includes
mediating between wealth and power
the way it once did.
It's now repetition of short, hot-button,
30-second, 28-second television ads.
We have to buy a lot of those ads.
Let's re-brand global warming,
as many of you have suggested.
I like ""climate crisis""
instead of ""climate collapse,""
but again, those of you
who are good at branding,
I need your help on this.
Somebody said the test
we're facing now, a scientist told me,
is whether the combination
of an opposable thumb
and a neocortex is a viable combination.




15:22


(Laughter)




15:24


That's really true.




15:29


I said the other night,
and I'll repeat now:
this is not a political issue.
Again, the Republicans here 
this shouldn't be partisan.
You have more influence
than some of us who are Democrats do.
This is an opportunity.
Not just this, but connected
to the ideas that are here,
to bring more coherence to them.
We are one.




15:55


Thank you very much, I appreciate it.




15:57


(Applause)",2017-06-13 11:35:45.317759
2780,Shah Rukh Khan,"Thoughts on humanity, fame and love","0:14


Namaskar.




 0:16


I'm a movie star, I'm 51 years of age,
and I don't use Botox as yet.




 0:22


(Laughter)




 0:24


So I'm clean, but I do behave like you saw
like a 21-year-old in my movies.
Yeah, I do that.
I sell dreams, and I peddle love
to millions of people back home in India
who assume that I'm
the best lover in the world.




 0:37


(Laughter)




 0:41


If you don't tell anyone,
I'm going to tell you I'm not,
but I never let that assumption go away.




 0:45


(Laughter)




 0:46


I've also been made to understand
there are lots of you here
who haven't seen my work,
and I feel really sad for you.




 0:52


(Laughter)




 0:54


(Applause)




 0:59


That doesn't take away from the fact
that I'm completely self-obsessed,
as a movie star should be.




 1:04


(Laughter)




 1:05


That's when my friends,
Chris and Juliet called me here
to speak about the future ""you.""
Naturally, it follows I'm going
to speak about the present me.




 1:13


(Laughter)




 1:18


Because I truly believe
that humanity is a lot like me.




 1:21


(Laughter)




 1:22


It is. It is.
It's an aging movie star,
grappling with all
the newness around itself,
wondering whether
it got it right in the first place,
and still trying to find a way
to keep on shining regardless.




 1:36


I was born in a refugee colony
in the capital city of India, New Delhi.
And my father was a freedom fighter.
My mother was, well,
just a fighter like mothers are.
And much like the original homo sapiens,
we struggled to survive.
When I was in my early 20s,
I lost both my parents,
which I must admit
seems a bit careless of me now,
but 




 2:02


(Laughter)




 2:08


I do remember the night my father died,
and I remember the driver of a neighbor
who was driving us to the hospital.
He mumbled something
about ""dead people don't tip so well""
and walked away into the dark.
And I was only 14 then,
and I put my father's dead body
in the back seat of the car,
and my mother besides me,
I started driving back
from the hospital to the house.
And in the middle of her quiet crying,
my mother looked at me and she said,
""Son, when did you learn to drive?""
And I thought about it
and realized, and I said to my mom,
""Just now, Mom.""




 2:41


(Laughter)




 2:43


So from that night onwards,
much akin to humanity in its adolescence,
I learned the crude tools of survival.
And the framework of life was
very, very simple then, to be honest.
You know, you just ate what you got
and did whatever you were told to do.
I thought celiac was a vegetable,
and vegan, of course, was Mr. Spock's
lost comrade in ""Star Trek.""




 3:08


(Laughter)




 3:09


You married the first girl that you dated,
and you were a techie if you could fix
the carburetor in your car.
I really thought that gay was
a sophisticated English word for happy.
And Lesbian, of course, was the capital
of Portugal, as you all know.




 3:26


(Laughter)




 3:27


Where was I?
We relied on systems
created through the toil and sacrifice
of generations before
to protect us,
and we felt that governments
actually worked for our betterment.
Science was simple and logical,
Apple was still then just a fruit
owned by Eve first and then Newton,
not by Steve Jobs, until then.
And ""Eureka!"" was what you screamed
when you wanted
to run naked on the streets.
You went wherever life took you for work,
and people were mostly welcoming of you.
Migration was a term then
still reserved for Siberian cranes,
not human beings.
Most importantly, you were who you were
and you said what you thought.




 4:14


Then in my late 20s,
I shifted to the sprawling
metropolis of Mumbai,
and my framework,
like the newly industrialized
aspirational humanity,
began to alter.
In the urban rush for a new,
more embellished survival,
things started to look a little different.
I met people who had descended
from all over the world,
faces, races, genders, money-lenders.
Definitions became more and more fluid.
Work began to define you at that time
in an overwhelmingly equalizing manner,
and all the systems
started to feel less reliable to me,
almost too thick to hold on
to the diversity of mankind
and the human need to progress and grow.
Ideas were flowing
with more freedom and speed.
And I experienced the miracle
of human innovation and cooperation,
and my own creativity,
when supported by the resourcefulness
of this collective endeavor,
catapulted me into superstardom.




 5:16


I started to feel that I had arrived,
and generally, by the time I was 40,
I was really, really flying.
I was all over the place.
You know? I'd done 50 films by then
and 200 songs,
and I'd been knighted by the Malaysians.
I had been given the highest civil honor
by the French government,
the title of which for the life of me
I can't pronounce even until now.




 5:36


(Laughter)




 5:37


I'm sorry, France, and thank you,
France, for doing that.
But much bigger than that,
I got to meet Angelina Jolie 




 5:44


(Laughter)




 5:47


for two and a half seconds.




 5:48


(Laughter)




 5:50


And I'm sure she also remembers
that encounter somewhere.
OK, maybe not.
And I sat next to Hannah Montana
on a round dinner table
with her back towards me most of the time.
Like I said, I was flying,
from Miley to Jolie,
and humanity was soaring with me.
We were both pretty much
flying off the handle, actually.




 6:10


And then you all know what happened.
The internet happened.
I was in my late 40s,
and I started tweeting
like a canary in a birdcage
and assuming that, you know,
people who peered into my world
would admire it
for the miracle I believed it to be.
But something else
awaited me and humanity.
You know, we had expected
an expansion of ideas and dreams
with the enhanced
connectivity of the world.
We had not bargained
for the village-like enclosure of thought,
of judgment, of definition
that flowed from the same place
that freedom and revolution
was taking place in.
Everything I said took a new meaning.
Everything I did  good, bad, ugly 
was there for the world
to comment upon and judge.
As a matter of fact,
everything I didn't say or do also
met with the same fate.




 7:07


Four years ago,
my lovely wife Gauri and me
decided to have a third child.
It was claimed on the net
that he was the love child
of our first child
who was 15 years old.
Apparently, he had sown
his wild oats with a girl
while driving her car in Romania.
And yeah, there was
a fake video to go with it.
And we were so disturbed as a family.
My son, who is 19 now,
even now when you say ""hello"" to him,
he just turns around and says,
""But bro, I didn't even have
a European driving license.""




 7:41


(Laughter)




 7:44


Yeah.
In this new world,
slowly, reality became virtual
and virtual became real,
and I started to feel
that I could not be who I wanted to be
or say what I actually thought,
and humanity at this time
completely identified with me.
I think both of us
were going through our midlife crisis,
and humanity, like me,
was becoming an overexposed prima donna.
I started to sell everything,
from hair oil to diesel generators.
Humanity was buying everything
from crude oil to nuclear reactors.
You know, I even tried
to get into a skintight superhero suit
to reinvent myself.
I must admit I failed miserably.
And just an aside I want to say
on behalf of all the Batmen, Spider-Men
and Supermen of the world,
you have to commend them,
because it really hurts in the crotch,
that superhero suit.




 8:39


(Laughter)




 8:40


Yeah, I'm being honest.
I need to tell you this here.
Really.
And accidentally, I happened
to even invent a new dance form
which I didn't realize,
and it became a rage.
So if it's all right,
and you've seen a bit of me,
so I'm quite shameless, I'll show you.
It was called the Lungi dance.
So if it's all right, I'll just show you.
I'm talented otherwise.




 9:00


(Cheers)




 9:02


So it went something like this.




 9:04


Lungi dance. Lungi dance.
Lungi dance. Lungi dance.
Lungi dance. Lungi dance.
Lungi dance. Lungi dance.
Lungi dance. Lungi dance.
Lungi dance. Lungi.




 9:12


That's it. It became a rage.




 9:14


(Cheers)




 9:15


It really did.
Like you notice, nobody could make
any sense of what was happening except me,
and I didn't give a damn, really,
because the whole world,
and whole humanity,
seemed as confused and lost as I was.
I didn't give up then.
I even tried to reconstruct
my identity on the social media
like everyone else does.
I thought if I put on
philosophical tweets out there
people will think I'm with it,
but some of the responses I got
from those tweets
were extremely confusing acronyms
which I didn't understand. You know?
ROFL, LOL.
""Adidas,"" somebody wrote back
to one of my more thought-provoking tweets
and I was wondering
why would you name a sneaker,
I mean, why would you write back
the name of a sneaker to me?
And I asked my 16-year-old daughter,
and she enlightened me.
""Adidas"" now means
""All day I dream about sex.""




10:03


(Laughter)




10:05


Really.
I didn't know if you know that.
So I wrote back,
""WTF"" in bold to Mr. Adidas,
thanking secretly that some acronyms
and things won't change at all.
WTF.




10:21


But here we are.
I am 51 years old, like I told you,
and mind-numbing acronyms notwithstanding,
I just want to tell you
if there has been a momentous time
for humanity to exist,
it is now,
because the present you is brave.
The present you is hopeful.
The present you
is innovative and resourceful,
and of course, the present you
is annoyingly indefinable.
And in this spell-binding,
imperfect moment of existence,
feeling a little brave
just before I came here,
I decided to take
a good, hard look at my face.
And I realized that I'm beginning
to look more and more
like the wax statue of me
at Madame Tussaud's.




11:04


(Laughter)




11:07


Yeah, and in that moment of realization,
I asked the most central
and pertinent question to humanity and me:
Do I need to fix my face?
Really. I'm an actor, like I told you,
a modern expression of human creativity.
The land I come from
is the source of inexplicable
but very simple spirituality.
In its immense generosity,
India decided somehow
that I, the Muslim son
of a broke freedom fighter
who accidentally ventured
into the business of selling dreams,
should become its king of romance,
the ""Badhshah of Bollywood,""
the greatest lover
the country has ever seen ...
with this face.
Yeah.




12:01


(Laughter)




12:02


Which has alternately
been described as ugly, unconventional,
and strangely, not chocolatey enough.




12:07


(Laughter)




12:12


The people of this ancient land
embraced me in their limitless love,
and I've learned from these people
that neither power nor poverty
can make your life more magical
or less tortuous.
I've learned from the people of my country
that the dignity of a life,
a human being, a culture,
a religion, a country
actually resides in its ability
for grace and compassion.
I've learned that whatever moves you,
whatever urges you to create, to build,
whatever keeps you from failing,
whatever helps you survive,
is perhaps the oldest and the simplest
emotion known to mankind,
and that is love.
A mystic poet from my land famously wrote,




13:01


(Recites poem in Hindi)




13:12


(Poem ends)




13:13


Which loosely translates
into that whatever 
yeah, if you know Hindi,
please clap, yeah.




13:18


(Applause)




13:19


It's very difficult to remember.
Which loosely translates
into actually saying
that all the books of knowledge
that you might read
and then go ahead
and impart your knowledge
through innovation,
through creativity, through technology,
but mankind will never be
the wiser about its future
unless it is coupled with a sense of love
and compassion for their fellow beings.
The two and a half alphabets
which form the word "",""
which means ""love,""
if you are able to understand that
and practice it,
that itself is enough
to enlighten mankind.
So I truly believe the future ""you""
has to be a you that loves.
Otherwise it will cease to flourish.
It will perish in its own self-absorption.




14:09


So you may use your power
to build walls
and keep people outside,
or you may use it to break barriers
and welcome them in.
You may use your faith
to make people afraid
and terrify them into submission,
or you can use it
to give courage to people
so they rise to the greatest
heights of enlightenment.
You can use your energy
to build nuclear bombs
and spread the darkness of destruction,
or you can use it to spread
the joy of light to millions.
You may filthy up the oceans callously
and cut down all the forests.
You can destroy the ecology,
or turn to them with love
and regenerate life
from the waters and trees.
You may land on Mars
and build armed citadels,
or you may look for life-forms and species
to learn from and respect.
And you can use
all the moneys we all have earned
to wage futile wars
and give guns in the hands
of little children
to kill each other with,
or you can use it
to make more food
to fill their stomachs with.




15:25


My country has taught me
the capacity for a human being to love
is akin to godliness.
It shines forth in a world
which civilization, I think,
already has tampered too much with.
In the last few days,
the talks here, the wonderful people
coming and showing their talent,
talking about individual achievements,
the innovation, the technology,
the sciences, the knowledge
we are gaining by being here
in the presence of TED Talks
and all of you
are reasons enough
for us to celebrate the future ""us.""
But within that celebration
the quest to cultivate
our capacity for love and compassion
has to assert itself,
has to assert itself,
just as equally.




16:14


So I believe the future ""you""
is an infinite you.
It's called a chakra
in India, like a circle.
It ends where it begins from
to complete itself.
A you that perceives
time and space differently
understands both
your unimaginable
and fantastic importance
and your complete unimportance
in the larger context of the universe.
A you that returns back
to the original innocence of humanity,
which loves from the purity of heart,
which sees from the eyes of truth,
which dreams from the clarity
of an untampered mind.




17:07


The future ""you"" has to be
like an aging movie star
who has been made to believe
that there is a possibility
of a world which is completely,
wholly, self-obsessively
in love with itself.
A world  really, it has to be a you
to create a world
which is its own best lover.
That I believe, ladies and gentlemen,
should be the future ""you.""




17:34


Thank you very much.
Shukriya.




17:37


(Applause)




17:39


Thank you.




17:40


(Applause)




17:43


Thank you.




17:44


(Applause)",2017-06-13 11:35:45.318036
2781,Stuart Russell,3 principles for creating safer AI,"0:11


This is Lee Sedol.
Lee Sedol is one of the world's
greatest Go players,
and he's having what my friends
in Silicon Valley call
a ""Holy Cow"" moment 




 0:21


(Laughter)




 0:22


a moment where we realize
that AI is actually progressing
a lot faster than we expected.
So humans have lost on the Go board.
What about the real world?




 0:32


Well, the real world is much bigger,
much more complicated than the Go board.
It's a lot less visible,
but it's still a decision problem.
And if we think about some
of the technologies
that are coming down the pike ...
Noriko [Arai] mentioned that reading
is not yet happening in machines,
at least with understanding.
But that will happen,
and when that happens,
very soon afterwards,
machines will have read everything
that the human race has ever written.
And that will enable machines,
along with the ability to look
further ahead than humans can,
as we've already seen in Go,
if they also have access
to more information,
they'll be able to make better decisions
in the real world than we can.
So is that a good thing?
Well, I hope so.




 1:25


Our entire civilization,
everything that we value,
is based on our intelligence.
And if we had access
to a lot more intelligence,
then there's really no limit
to what the human race can do.
And I think this could be,
as some people have described it,
the biggest event in human history.
So why are people saying things like this,
that AI might spell the end
of the human race?
Is this a new thing?
Is it just Elon Musk and Bill Gates
and Stephen Hawking?




 2:00


Actually, no. This idea
has been around for a while.
Here's a quotation:
""Even if we could keep the machines
in a subservient position,
for instance, by turning off the power
at strategic moments"" 
and I'll come back to that
""turning off the power"" idea later on 
""we should, as a species,
feel greatly humbled.""
So who said this?
This is Alan Turing in 1951.
Alan Turing, as you know,
is the father of computer science
and in many ways,
the father of AI as well.
So if we think about this problem,
the problem of creating something
more intelligent than your own species,
we might call this ""the gorilla problem,""
because gorillas' ancestors did this
a few million years ago,
and now we can ask the gorillas:
Was this a good idea?




 2:48


So here they are having a meeting
to discuss whether it was a good idea,
and after a little while,
they conclude, no,
this was a terrible idea.
Our species is in dire straits.
In fact, you can see the existential
sadness in their eyes.




 3:03


(Laughter)




 3:05


So this queasy feeling that making
something smarter than your own species
is maybe not a good idea 
what can we do about that?
Well, really nothing,
except stop doing AI,
and because of all
the benefits that I mentioned
and because I'm an AI researcher,
I'm not having that.
I actually want to be able
to keep doing AI.




 3:29


So we actually need to nail down
the problem a bit more.
What exactly is the problem?
Why is better AI possibly a catastrophe?




 3:38


So here's another quotation:
""We had better be quite sure
that the purpose put into the machine
is the purpose which we really desire.""
This was said by Norbert Wiener in 1960,
shortly after he watched
one of the very early learning systems
learn to play checkers
better than its creator.
But this could equally have been said
by King Midas.
King Midas said, ""I want everything
I touch to turn to gold,""
and he got exactly what he asked for.
That was the purpose
that he put into the machine,
so to speak,
and then his food and his drink
and his relatives turned to gold
and he died in misery and starvation.
So we'll call this
""the King Midas problem""
of stating an objective
which is not, in fact,
truly aligned with what we want.
In modern terms, we call this
""the value alignment problem.""




 4:36


Putting in the wrong objective
is not the only part of the problem.
There's another part.
If you put an objective into a machine,
even something as simple as,
""Fetch the coffee,""
the machine says to itself,
""Well, how might I fail
to fetch the coffee?
Someone might switch me off.
OK, I have to take steps to prevent that.
I will disable my 'off' switch.
I will do anything to defend myself
against interference
with this objective
that I have been given.""
So this single-minded pursuit
in a very defensive mode
of an objective that is, in fact,
not aligned with the true objectives
of the human race 
that's the problem that we face.
And in fact, that's the high-value
takeaway from this talk.
If you want to remember one thing,
it's that you can't fetch
the coffee if you're dead.




 5:27


(Laughter)




 5:28


It's very simple. Just remember that.
Repeat it to yourself three times a day.




 5:32


(Laughter)




 5:34


And in fact, this is exactly the plot
of ""2001: [A Space Odyssey]""
HAL has an objective, a mission,
which is not aligned
with the objectives of the humans,
and that leads to this conflict.
Now fortunately, HAL
is not superintelligent.
He's pretty smart,
but eventually Dave outwits him
and manages to switch him off.
But we might not be so lucky.
So what are we going to do?




 6:11


I'm trying to redefine AI
to get away from this classical notion
of machines that intelligently
pursue objectives.
There are three principles involved.
The first one is a principle
of altruism, if you like,
that the robot's only objective
is to maximize the realization
of human objectives,
of human values.
And by values here I don't mean
touchy-feely, goody-goody values.
I just mean whatever it is
that the human would prefer
their life to be like.
And so this actually violates Asimov's law
that the robot has to protect
its own existence.
It has no interest in preserving
its existence whatsoever.




 6:56


The second law is a law
of humility, if you like.
And this turns out to be really
important to make robots safe.
It says that the robot does not know
what those human values are,
so it has to maximize them,
but it doesn't know what they are.
And that avoids this problem
of single-minded pursuit
of an objective.
This uncertainty turns out to be crucial.




 7:20


Now, in order to be useful to us,
it has to have some idea of what we want.
It obtains that information primarily
by observation of human choices,
so our own choices reveal information
about what it is that we prefer
our lives to be like.
So those are the three principles.
Let's see how that applies
to this question of:
""Can you switch the machine off?""
as Turing suggested.




 7:48


So here's a PR2 robot.
This is one that we have in our lab,
and it has a big red ""off"" switch
right on the back.
The question is: Is it
going to let you switch it off?
If we do it the classical way,
we give it the objective of, ""Fetch
the coffee, I must fetch the coffee,
I can't fetch the coffee if I'm dead,""
so obviously the PR2
has been listening to my talk,
and so it says, therefore,
""I must disable my 'off' switch,
and probably taser all the other
people in Starbucks
who might interfere with me.""




 8:18


(Laughter)




 8:20


So this seems to be inevitable, right?
This kind of failure mode
seems to be inevitable,
and it follows from having
a concrete, definite objective.




 8:29


So what happens if the machine
is uncertain about the objective?
Well, it reasons in a different way.
It says, ""OK, the human
might switch me off,
but only if I'm doing something wrong.
Well, I don't really know what wrong is,
but I know that I don't want to do it.""
So that's the first and second
principles right there.
""So I should let the human switch me off.""
And in fact you can calculate
the incentive that the robot has
to allow the human to switch it off,
and it's directly tied to the degree
of uncertainty about
the underlying objective.




 9:04


And then when the machine is switched off,
that third principle comes into play.
It learns something about the objectives
it should be pursuing,
because it learns that
what it did wasn't right.
In fact, we can, with suitable use
of Greek symbols,
as mathematicians usually do,
we can actually prove a theorem
that says that such a robot
is provably beneficial to the human.
You are provably better off
with a machine that's designed in this way
than without it.
So this is a very simple example,
but this is the first step
in what we're trying to do
with human-compatible AI.




 9:41


Now, this third principle,
I think is the one that you're probably
scratching your head over.
You're probably thinking, ""Well,
you know, I behave badly.
I don't want my robot to behave like me.
I sneak down in the middle of the night
and take stuff from the fridge.
I do this and that.""
There's all kinds of things
you don't want the robot doing.
But in fact, it doesn't
quite work that way.
Just because you behave badly
doesn't mean the robot
is going to copy your behavior.
It's going to understand your motivations
and maybe help you resist them,
if appropriate.
But it's still difficult.
What we're trying to do, in fact,
is to allow machines to predict
for any person and for any possible life
that they could live,
and the lives of everybody else:
Which would they prefer?
And there are many, many
difficulties involved in doing this;
I don't expect that this
is going to get solved very quickly.
The real difficulties, in fact, are us.




10:43


As I have already mentioned,
we behave badly.
In fact, some of us are downright nasty.
Now the robot, as I said,
doesn't have to copy the behavior.
The robot does not have
any objective of its own.
It's purely altruistic.
And it's not designed just to satisfy
the desires of one person, the user,
but in fact it has to respect
the preferences of everybody.
So it can deal with a certain
amount of nastiness,
and it can even understand
that your nastiness, for example,
you may take bribes as a passport official
because you need to feed your family
and send your kids to school.
It can understand that;
it doesn't mean it's going to steal.
In fact, it'll just help you
send your kids to school.




11:27


We are also computationally limited.
Lee Sedol is a brilliant Go player,
but he still lost.
So if we look at his actions,
he took an action that lost the game.
That doesn't mean he wanted to lose.
So to understand his behavior,
we actually have to invert
through a model of human cognition
that includes our computational
limitations  a very complicated model.
But it's still something
that we can work on understanding.




11:56


Probably the most difficult part,
from my point of view as an AI researcher,
is the fact that there are lots of us,
and so the machine has to somehow
trade off, weigh up the preferences
of many different people,
and there are different ways to do that.
Economists, sociologists,
moral philosophers have understood that,
and we are actively
looking for collaboration.




12:19


Let's have a look and see what happens
when you get that wrong.
So you can have
a conversation, for example,
with your intelligent personal assistant
that might be available
in a few years' time.
Think of a Siri on steroids.
So Siri says, ""Your wife called
to remind you about dinner tonight.""
And of course, you've forgotten.
""What? What dinner?
What are you talking about?""




12:41


""Uh, your 20th anniversary at 7pm.""




12:47


""I can't do that. I'm meeting
with the secretary-general at 7:30.
How could this have happened?""




12:53


""Well, I did warn you, but you overrode
my recommendation.""




12:59


""Well, what am I going to do?
I can't just tell him I'm too busy.""




13:03


""Don't worry. I arranged
for his plane to be delayed.""




13:06


(Laughter)




13:09


""Some kind of computer malfunction.""




13:11


(Laughter)




13:12


""Really? You can do that?""




13:15


""He sends his profound apologies
and looks forward to meeting you
for lunch tomorrow.""




13:20


(Laughter)




13:21


So the values here 
there's a slight mistake going on.
This is clearly following my wife's values
which is ""Happy wife, happy life.""




13:31


(Laughter)




13:32


It could go the other way.
You could come home
after a hard day's work,
and the computer says, ""Long day?""




13:39


""Yes, I didn't even have time for lunch.""




13:41


""You must be very hungry.""




13:42


""Starving, yeah.
Could you make some dinner?""




13:47


""There's something I need to tell you.""




13:49


(Laughter)




13:51


""There are humans in South Sudan
who are in more urgent need than you.""




13:56


(Laughter)




13:57


""So I'm leaving. Make your own dinner.""




13:59


(Laughter)




14:01


So we have to solve these problems,
and I'm looking forward
to working on them.




14:06


There are reasons for optimism.
One reason is,
there is a massive amount of data.
Because remember  I said
they're going to read everything
the human race has ever written.
Most of what we write about
is human beings doing things
and other people getting upset about it.
So there's a massive amount
of data to learn from.




14:22


There's also a very
strong economic incentive
to get this right.
So imagine your domestic robot's at home.
You're late from work again
and the robot has to feed the kids,
and the kids are hungry
and there's nothing in the fridge.
And the robot sees the cat.




14:38


(Laughter)




14:39


And the robot hasn't quite learned
the human value function properly,
so it doesn't understand
the sentimental value of the cat outweighs
the nutritional value of the cat.




14:50


(Laughter)




14:51


So then what happens?
Well, it happens like this:
""Deranged robot cooks kitty
for family dinner.""
That one incident would be the end
of the domestic robot industry.
So there's a huge incentive
to get this right
long before we reach
superintelligent machines.




15:11


So to summarize:
I'm actually trying to change
the definition of AI
so that we have provably
beneficial machines.
And the principles are:
machines that are altruistic,
that want to achieve only our objectives,
but that are uncertain
about what those objectives are,
and will watch all of us
to learn more about what it is
that we really want.
And hopefully in the process,
we will learn to be better people.
Thank you very much.




15:38


(Applause)




15:41


Chris Anderson: So interesting, Stuart.
We're going to stand here a bit
because I think they're setting up
for our next speaker.




15:48


A couple of questions.
So the idea of programming in ignorance
seems intuitively really powerful.
As you get to superintelligence,
what's going to stop a robot
reading literature and discovering
this idea that knowledge
is actually better than ignorance
and still just shifting its own goals
and rewriting that programming?




16:08


Stuart Russell: Yes, so we want
it to learn more, as I said,
about our objectives.
It'll only become more certain
as it becomes more correct,
so the evidence is there
and it's going to be designed
to interpret it correctly.
It will understand, for example,
that books are very biased
in the evidence they contain.
They only talk about kings and princes
and elite white male people doing stuff.
So it's a complicated problem,
but as it learns more about our objectives
it will become more and more useful to us.




16:45


CA: And you couldn't
just boil it down to one law,
you know, hardwired in:
""if any human ever tries to switch me off,
I comply. I comply.""




16:54


SR: Absolutely not.
That would be a terrible idea.
So imagine that you have
a self-driving car
and you want to send your five-year-old
off to preschool.
Do you want your five-year-old
to be able to switch off the car
while it's driving along?
Probably not.
So it needs to understand how rational
and sensible the person is.
The more rational the person,
the more willing you are
to be switched off.
If the person is completely
random or even malicious,
then you're less willing
to be switched off.




17:23


CA: All right. Stuart, can I just say,
I really, really hope you
figure this out for us.
Thank you so much for that talk.
That was amazing.




17:29


SR: Thank you.




17:31


(Applause)",2017-06-13 11:35:45.318260
