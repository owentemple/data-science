{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src='img/logo.png'>\n",
    "<img src='img/title.png'>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Table of Contents\n",
    "* [Preprocessing](#Preprocessing)\n",
    "\t* [Setup](#Setup)\n",
    "\t* [Example workflow with `MinMaxScaler`](#Example-workflow-with-MinMaxScaler)\n",
    "\t* [Scaling training and test data the same way](#Scaling-training-and-test-data-the-same-way)\n",
    "\t* [The effect of preprocessing on supervised learning](#The-effect-of-preprocessing-on-supervised-learning)\n",
    "\t\t* [`preprocessing.scale`](#preprocessing.scale)\n",
    "* [Summary](#Summary)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hide_input": false
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "\n",
    "plt.rcParams['image.interpolation'] = \"none\"\n",
    "np.set_printoptions(precision=3)\n",
    "plt.rcParams['image.cmap'] = \"gray\"\n",
    "\n",
    "import src.mglearn as mglearn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "http://scikit-learn.org/stable/modules/classes.html#module-sklearn.preprocessing\n",
    "\n",
    "Preprocessing is often desirable for the following reasons:\n",
    " * Input data may have many dimensions and/or colinear dimensions, making it desirable to simplify the fitting problem by reducing the number of columns (features)\n",
    " * Input data may not have the statistical properties that are ideal for fitting\n",
    "\n",
    "The following cell shows how several different preprocessing scalers affect input feature statstics.  An overview of the [sklearn.preprocessing api can be found here](http://scikit-learn.org/stable/modules/classes.html#module-sklearn.preprocessing).\n",
    "\n",
    "The block beginning with:\n",
    "```\n",
    "\n",
    "for ax, scaler in zip(other_axes, [StandardScaler(), RobustScaler(),\n",
    "                                   MinMaxScaler(), Normalizer(norm='l2')]\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "is looping over 4 different scalers:\n",
    " * `StandardScaler`: [Remove mean and create unit variance](http://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.StandardScaler.html)\n",
    " * `RobustScaler`: [Scaling robust to outliers in input data](http://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.RobustScaler)\n",
    " * `MinMaxScaler`: [Scale to the min / max range of each feature](http://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.MinMaxScaler.html#sklearn.preprocessing.MinMaxScaler)\n",
    " * `Normalizer(norm='l2')`: [Normalize to a unit norm](http://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.Normalizer.html#sklearn.preprocessing.Normalizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hide_input": false
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from sklearn.datasets import make_blobs\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler, Normalizer, RobustScaler\n",
    "from src.mglearn.plot_helpers import cm2\n",
    "\n",
    "\n",
    "\n",
    "X, y = make_blobs(n_samples=50, centers=2, random_state=4, cluster_std=1)\n",
    "X += 3\n",
    "\n",
    "plt.figure(figsize=(15, 8))\n",
    "main_ax = plt.subplot2grid((2, 4), (0, 0), rowspan=2, colspan=2)\n",
    "\n",
    "main_ax.scatter(X[:, 0], X[:, 1], c=y, cmap=cm2, s=60)\n",
    "maxx = np.abs(X[:, 0]).max()\n",
    "maxy = np.abs(X[:, 1]).max()\n",
    "\n",
    "main_ax.set_xlim(-maxx + 1, maxx + 1)\n",
    "main_ax.set_ylim(-maxy + 1, maxy + 1)\n",
    "main_ax.set_title(\"Original Data\")\n",
    "other_axes = [plt.subplot2grid((2, 4), (i, j)) for j in range(2, 4) for i in range(2)]\n",
    "\n",
    "for ax, scaler in zip(other_axes, [StandardScaler(), RobustScaler(),\n",
    "                                   MinMaxScaler(), Normalizer(norm='l2')]):\n",
    "    X_ = scaler.fit_transform(X)\n",
    "    ax.scatter(X_[:, 0], X_[:, 1], c=y, cmap=cm2, s=60)\n",
    "    ax.set_xlim(-2, 2)\n",
    "    ax.set_ylim(-2, 2)\n",
    "    ax.set_title(type(scaler).__name__)\n",
    "\n",
    "other_axes.append(main_ax)\n",
    "\n",
    "for ax in other_axes:\n",
    "    ax.spines['left'].set_position('center')\n",
    "    ax.spines['right'].set_color('none')\n",
    "    ax.spines['bottom'].set_position('center')\n",
    "    ax.spines['top'].set_color('none')\n",
    "    ax.xaxis.set_ticks_position('bottom')\n",
    "    ax.yaxis.set_ticks_position('left')\n",
    "\n",
    "plt.suptitle(\"scaling_data\")\n",
    "plt.show();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example workflow with `MinMaxScaler`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This example uses a single training data / test data split, with a `MinMaxScaler` putting features in the `(0, 1)` range."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_breast_cancer\n",
    "from sklearn.model_selection import train_test_split\n",
    "cancer = load_breast_cancer()\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(cancer.data, cancer.target,\n",
    "                                                    random_state=1)\n",
    "print(X_train.shape)\n",
    "print(X_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import MinMaxScaler\n",
    "scaler = MinMaxScaler()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler.fit(X_train)    # first .fit finds stats needed for scaling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Stats of the data before / after MinMaxScaler\n",
    "X_train_scaled = scaler.transform(X_train)\n",
    "print(\"transformed shape: %s\" % (X_train_scaled.shape,))\n",
    "print(\"per-feature minimum before scaling:\\n %s\" % X_train.min(axis=0))\n",
    "print(\"per-feature maximum before scaling:\\n %s\" % X_train.max(axis=0))\n",
    "print(\"per-feature minimum after scaling:\\n %s\" % X_train_scaled.min(axis=0))\n",
    "print(\"per-feature maximum after scaling:\\n %s\" % X_train_scaled.max(axis=0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# transform test data\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "# print test data properties after scaling\n",
    "print(\"per-feature minimum after scaling: %s\" % X_test_scaled.min(axis=0))\n",
    "print(\"per-feature maximum after scaling: %s\" % X_test_scaled.max(axis=0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Scaling training and test data the same way"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Make sure to call the scaler's `.fit` method separately for training and test data, so the data are not scaled by a different data set's statistics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import make_blobs\n",
    "# make synthetic data\n",
    "X, _ = make_blobs(n_samples=50, centers=5, random_state=4, cluster_std=2)\n",
    "# split it into training and test set\n",
    "X_train, X_test = train_test_split(X, random_state=5, test_size=.1)\n",
    "\n",
    "# plot the training and test set\n",
    "fig, axes = plt.subplots(1, 3, figsize=(13, 4))\n",
    "axes[0].scatter(X_train[:, 0], X_train[:, 1],\n",
    "                c='b', label=\"training set\", s=60)\n",
    "axes[0].scatter(X_test[:, 0], X_test[:, 1], marker='^',\n",
    "                c='r', label=\"test set\", s=60)\n",
    "axes[0].legend(loc='upper left')\n",
    "axes[0].set_title(\"original data\")\n",
    "\n",
    "# scale the data using MinMaxScaler\n",
    "scaler = MinMaxScaler()\n",
    "scaler.fit(X_train)\n",
    "X_train_scaled = scaler.transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "# visualize the properly scaled data\n",
    "axes[1].scatter(X_train_scaled[:, 0], X_train_scaled[:, 1],\n",
    "                c='b', label=\"training set\", s=60)\n",
    "axes[1].scatter(X_test_scaled[:, 0], X_test_scaled[:, 1], marker='^',\n",
    "                c='r', label=\"test set\", s=60)\n",
    "axes[1].set_title(\"scaled data\")\n",
    "\n",
    "# rescale the test set separately, so that test set min is 0 and test set max is 1\n",
    "# DO NOT DO THIS! For illustration purposes only\n",
    "test_scaler = MinMaxScaler()\n",
    "test_scaler.fit(X_test)\n",
    "X_test_scaled_badly = test_scaler.transform(X_test)\n",
    "\n",
    "# visualize wrongly scaled data\n",
    "axes[2].scatter(X_train_scaled[:, 0], X_train_scaled[:, 1],\n",
    "                c='b', label=\"training set\", s=60)\n",
    "axes[2].scatter(X_test_scaled_badly[:, 0], X_test_scaled_badly[:, 1], marker='^',\n",
    "                c='r', label=\"test set\", s=60)\n",
    "axes[2].set_title(\"improperly scaled data\");"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The effect of preprocessing on supervised learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The cells below show that min / max scaling or standardization can improve the predictive value of a support vector classifier for the cancer data set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.svm import SVC\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(cancer.data, cancer.target,\n",
    "                                                    random_state=0)\n",
    "\n",
    "svm = SVC(C=100)\n",
    "svm.fit(X_train, y_train)\n",
    "print(svm.score(X_test, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# preprocessing using 0-1 scaling\n",
    "scaler = MinMaxScaler()\n",
    "scaler.fit(X_train)\n",
    "X_train_scaled = scaler.transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "# learning an SVM on the scaled training data\n",
    "svm.fit(X_train_scaled, y_train)\n",
    "# scoring on the scaled test set\n",
    "\n",
    "svm.score(X_test_scaled, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# preprocessing using zero mean and unit variance scaling\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "scaler = StandardScaler()\n",
    "scaler.fit(X_train)\n",
    "X_train_scaled = scaler.transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "# learning an SVM on the scaled training data\n",
    "svm.fit(X_train_scaled, y_train)\n",
    "# scoring on the scaled test set\n",
    "svm.score(X_test_scaled, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### `preprocessing.scale`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Scaling with function instead of a class.  The default behavior removes the mean and scales standard deviation to 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import preprocessing\n",
    "import numpy as np\n",
    "train = np.c_[np.random.normal(10, 2, 100),\n",
    "              np.random.normal(size=100),\n",
    "              np.random.normal(0, 5, 100)]\n",
    "\n",
    "# with_mean/with_std True/False (default True)\n",
    "scaled = preprocessing.scale(train)\n",
    "print(np.allclose(0.0, scaled.mean(axis=0)), np.allclose(1.0, scaled.std(axis=0)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Summary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this notebook, we reviewed the following topics in preparation for more advanced topics:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* [Preprocessing](#Preprocessing)\n",
    "* [Example workflow with `MinMaxScaler`](#Example-workflow-with-MinMaxScaler)\n",
    "* [Scaling training and test data the same way](#Scaling-training-and-test-data-the-same-way)\n",
    "* [The effect of preprocessing on supervised learning](#The-effect-of-preprocessing-on-supervised-learning)\n",
    "* [`preprocessing.scale`](#preprocessing.scale)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a href='Scaling_and_Normalization_Exercises.ipynb' class='btn btn-primary btn-lg'>Exercises</a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src='img/copyright.png'>"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python",
   "pygments_lexer": "ipython3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
