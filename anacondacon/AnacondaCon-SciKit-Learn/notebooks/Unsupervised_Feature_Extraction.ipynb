{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src='img/logo.png'>\n",
    "<img src='img/title.png'>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Unsupervised Learning and Feature Extraction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Table of Contents\n",
    "* [Unsupervised Learning and Feature Extraction](#Unsupervised-Learning-and-Feature-Extraction)\n",
    "\t* [Dimensionality Reduction, Feature Extraction and Manifold Learning](#Dimensionality-Reduction,-Feature-Extraction-and-Manifold-Learning)\n",
    "\t\t* [Principal Component Analysis (PCA)](#Principal-Component-Analysis-%28PCA%29)\n",
    "\t\t\t* [Applying PCA to the cancer dataset for visualization](#Applying-PCA-to-the-cancer-dataset-for-visualization)\n",
    "\t\t* [`PCA`: rotation, *sphere*ing,  and whitening example](#PCA:-rotation,-*sphere*ing,--and-whitening-example)\n",
    "\t\t\t* [Center the data](#Center-the-data)\n",
    "\t\t\t* [PCA Take 1: Rotation from Pricipal (Data) Axes to Standard Axes](#PCA-Take-1:-Rotation-from-Pricipal-%28Data%29-Axes-to-Standard-Axes)\n",
    "\t\t\t* [PCA Take 2:  *Sphere*ing (i.e., Whitening or IID Whitening)](#PCA-Take-2:--*Sphere*ing-%28i.e.,-Whitening-or-IID-Whitening%29)\n",
    "\t\t\t* [PCA Take 3:  On Standardized Data](#PCA-Take-3:--On-Standardized-Data)\n",
    "\t\t* [Another PCA Example](#Another-PCA-Example)\n",
    "\t\t* [Non-Negative Matrix Factorization (NMF)](#Non-Negative-Matrix-Factorization-%28NMF%29)\n",
    "\t\t\t* [Applying NMF to synthetic data](#Applying-NMF-to-synthetic-data)\n",
    "\t\t* [Manifold learning with t-SNE](#Manifold-learning-with-t-SNE)\n",
    "\t* [Independent Component Analysis](#Independent-Component-Analysis)\n",
    "\t\t* [Using a `t` distribution with small degrees of freedom (non-Gaussian)](#Using-a-t-distribution-with-small-degrees-of-freedom-%28non-Gaussian%29)\n",
    "* [Summary](#Summary)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hide_input": false
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "\n",
    "plt.rcParams['image.interpolation'] = \"none\"\n",
    "np.set_printoptions(precision=3)\n",
    "plt.rcParams['image.cmap'] = \"gray\"\n",
    "\n",
    "import src.mglearn as mglearn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dimensionality Reduction, Feature Extraction and Manifold Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's examine the breast cancer data set. There are many features with varying ranges of values.\n",
    "\n",
    "* What subset are most useful?\n",
    "* Which ones exbhibit colinearity?\n",
    "\n",
    "These can be hard questions to answer. It would be better to reduce the dimensionality of the features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_breast_cancer\n",
    "from sklearn.model_selection import train_test_split\n",
    "cancer = load_breast_cancer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(15, 2, figsize=(10, 20))\n",
    "malignant = cancer.data[cancer.target == 0]\n",
    "benign = cancer.data[cancer.target == 1]\n",
    "\n",
    "ax = axes.ravel()\n",
    "\n",
    "for i in range(30):\n",
    "    _, bins = np.histogram(cancer.data[:, i], bins=50)\n",
    "    ax[i].hist(malignant[:, i], bins=bins, color='b', alpha=.5)\n",
    "    ax[i].hist(benign[:, i], bins=bins, color='r', alpha=.5)\n",
    "    ax[i].set_title(cancer.feature_names[i])\n",
    "    ax[i].set_yticks(())\n",
    "fig.tight_layout()\n",
    "plt.suptitle(\"cancer_histograms\");"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Principal Component Analysis (PCA)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Methods like PCA can simplify a colinear input matrix to a smaller set of representative columns (axes)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "PCA finds orthogonal axes that best explain variance in the input matrix. Scikit-learn has two PCA models, [sklearn.decomposition.PCA](http://scikit-learn.org/stable/modules/generated/sklearn.decomposition.PCA.html) and [sklearn.decomposition.IncrementalPCA](http://scikit-learn.org/stable/modules/generated/sklearn.decomposition.IncrementalPCA.html), with `IncrementalPCA` allowing fitting to large input matrices that are subsampled."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hide_input": false
   },
   "outputs": [],
   "source": [
    "mglearn.plots.plot_pca_illustration()\n",
    "plt.suptitle(\"pca_illustration\");"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Applying PCA to the cancer dataset for visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_breast_cancer\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "cancer = load_breast_cancer()\n",
    "\n",
    "scaler = StandardScaler()\n",
    "scaler.fit(cancer.data)\n",
    "X_scaled = scaler.transform(cancer.data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "# keep the first two principal components of the data\n",
    "pca = PCA(n_components=2)\n",
    "# fit PCA model to beast cancer data\n",
    "pca.fit(X_scaled)\n",
    "\n",
    "# transform data onto the first two principal components\n",
    "X_pca = pca.transform(X_scaled)\n",
    "print(\"Original shape: %s\" % str(X_scaled.shape))\n",
    "print(\"Reduced shape: %s\" % str(X_pca.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot fist vs second principal component, color by class\n",
    "plt.figure(figsize=(8, 8))\n",
    "plt.scatter(X_pca[:, 0], X_pca[:, 1], c=cancer.target, cmap=mglearn.tools.cm, s=60)\n",
    "plt.gca().set_aspect(\"equal\")\n",
    "plt.xlabel(\"First principal component\")\n",
    "plt.ylabel(\"Second principal component\");"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pca.components_.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(pca.components_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.matshow(pca.components_, cmap='viridis')\n",
    "plt.yticks([0, 1], [\"first component\", \"second component\"])\n",
    "plt.colorbar()\n",
    "plt.xticks(range(len(cancer.feature_names)),\n",
    "           cancer.feature_names, rotation=60, ha='left');\n",
    "plt.suptitle(\"pca_components_cancer\");"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### `PCA`: rotation, *sphere*ing,  and whitening example"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following series of cells show how preprocessing steps such as subtracting off the mean or calculating a z-score `(X - X.mean() / X.std())` can improve the fit of a PCA model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import numpy as np\n",
    "from sklearn import decomposition\n",
    "import matplotlib.pyplot as plt\n",
    "# helper function\n",
    "def examineData(data, ax=None, color=\"\"):\n",
    "    '''Helper to plot two columns and their covariance'''\n",
    "    print(\"Mean Vector:\")\n",
    "    print(np.mean(data, axis=0))\n",
    "    print(\"Covariance Matrix:\")\n",
    "    print(np.cov(data, rowvar=False))\n",
    "    \n",
    "    if not ax:\n",
    "        fig, ax = plt.subplots(1,1)\n",
    "    ax.plot(data.T[0], data.T[1], color + 'o')\n",
    "    ax.set_aspect('equal')\n",
    "    ax.set_xlim((-20,20))\n",
    "    ax.set_ylim((-20,20))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax1 = plt.subplots(1, 1)\n",
    "\n",
    "# create some sample data\n",
    "mean = [8,12]\n",
    "cov = [[2,3],[5,8]]\n",
    "data = np.random.multivariate_normal(mean, cov, 100)\n",
    "examineData(data, ax1, 'b')\n",
    "\n",
    "# center the data\n",
    "centered = data - np.mean(data,axis=0)\n",
    "examineData(centered, ax1, 'r')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax1 = plt.subplots(1, 1)\n",
    "\n",
    "# create some sample data\n",
    "mean = [8,12]\n",
    "cov = [[2,3],[5,8]]\n",
    "data = np.random.multivariate_normal(mean, cov, 100)\n",
    "examineData(data, ax1, 'b')\n",
    "\n",
    "# center the data\n",
    "centered = data - np.mean(data,axis=0)\n",
    "examineData(centered, ax1, 'r')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Center the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "centered = data - np.mean(data,axis=0)\n",
    "examineData(centered)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### PCA Take 1: Rotation from Pricipal (Data) Axes to Standard Axes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pca = decomposition.PCA(n_components=2)\n",
    "pca.fit(centered)\n",
    "rotated = pca.fit_transform(centered)\n",
    "examineData(rotated)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### PCA Take 2:  *Sphere*ing (i.e., Whitening or IID Whitening)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pca = decomposition.PCA(n_components=2, whiten=True)\n",
    "pca.fit(centered)\n",
    "sphered = pca.fit_transform(centered)\n",
    "examineData(sphered)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### PCA Take 3:  On Standardized Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(10,5))\n",
    "\n",
    "# see also:  scipy.stats.zscore\n",
    "zdata = (data-np.mean(data, axis=0)) / np.std(data, axis=0, ddof=1)\n",
    "examineData(zdata, ax1)\n",
    "\n",
    "pca = decomposition.PCA(n_components=2)\n",
    "pca.fit(zdata)\n",
    "rotated_z = pca.fit_transform(zdata)\n",
    "examineData(rotated_z, ax2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Another PCA Example"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`pandas.tools.plotting.scatter_matrix` is a good way to look at matrices with colinear variables.  Here the `scatter_matrix` shows 3 of 4 columns of the feature matrix are colinear and thus a good candidate for dimensionality reduction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mean = [8,12]\n",
    "cov = [[2,3],[5,8]]\n",
    "X, Y = np.random.multivariate_normal(mean, cov, 100).T\n",
    "expanded_data = np.c_[2*X + Y, .5*X+.5*Y, X**2 + Y, np.sqrt(X) + np.sin(Y)]\n",
    "centered = expanded_data - np.mean(expanded_data, axis=0)\n",
    "\n",
    "import pandas as pd\n",
    "pd.tools.plotting.scatter_matrix(pd.DataFrame(centered));"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pca = decomposition.PCA(n_components=2, whiten=True)\n",
    "pca.fit(centered)\n",
    "\n",
    "fmt = \"Component %d explains %f of variance\"\n",
    "print(\"\\n\".join(fmt % (idx, evr) for idx, evr in enumerate(pca.explained_variance_ratio_)))\n",
    "sphered = pca.fit_transform(centered)  # .fit() then .transform()\n",
    "examineData(sphered)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Non-Negative Matrix Factorization (NMF)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "NMF is a decomposition method solving the following matrix algebra equality on 3 positive matrices:\n",
    "```python\n",
    "V = W * H\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "where:\n",
    " * `V`'s shape is `(n, m)`, \n",
    " * `W`'s shape is `(n, n_components)`, and \n",
    " * `H`'s shape is `(n_components, m)`.\n",
    "\n",
    "[This SIAM conference presentation](https://www.siam.org/meetings/sdm11/park.pdf) provides more background on NMF and comparison to other decomposition methods.\n",
    "\n",
    "NMF with scikit-learn [is described here sklearn.decomposition.NMF](http://scikit-learn.org/stable/modules/generated/sklearn.decomposition.NMF.html)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Applying NMF to synthetic data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hide_input": false
   },
   "outputs": [],
   "source": [
    "mglearn.plots.plot_nmf_illustration()\n",
    "plt.suptitle(\"nmf_illustration\");"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Manifold learning with t-SNE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following cells show manifold learning with a better fit to the `digits` scikit-learn example data than PCA.  More information on manifold learning can be found in \n",
    "* [this comparison of manifold methods](http://scikit-learn.org/stable/auto_examples/manifold/plot_compare_methods.html)\n",
    "* [a manifold example with severed sphere](http://scikit-learn.org/stable/auto_examples/manifold/plot_manifold_sphere.html#sphx-glr-auto-examples-manifold-plot-manifold-sphere-py)\n",
    "\n",
    "Note [the full documentation on the manifold module](http://scikit-learn.org/stable/modules/manifold.html#manifold) describes a variety of useful estimators we do not cover in this notebook, such as [LocallyLinearEmbedding](http://scikit-learn.org/stable/modules/generated/sklearn.manifold.LocallyLinearEmbedding.html) and [SpectralEmbedding](http://scikit-learn.org/stable/modules/generated/sklearn.manifold.SpectralEmbedding.html)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_digits\n",
    "digits = load_digits()\n",
    "\n",
    "fig, axes = plt.subplots(2, 5, figsize=(10, 5),\n",
    "                         subplot_kw={'xticks':(), 'yticks': ()})\n",
    "for ax, img in zip(axes.ravel(), digits.images):\n",
    "    ax.imshow(img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# build a PCA model\n",
    "pca = PCA(n_components=2)\n",
    "pca.fit(digits.data)\n",
    "# transform the digits data onto the first two principal components\n",
    "digits_pca = pca.transform(digits.data)\n",
    "colors = [\"#476A2A\", \"#7851B8\", \"#BD3430\", \"#4A2D4E\", \"#875525\",\n",
    "          \"#A83683\", \"#4E655E\", \"#853541\", \"#3A3120\",\"#535D8E\"]\n",
    "plt.figure(figsize=(10, 10))\n",
    "plt.xlim(digits_pca[:, 0].min(), digits_pca[:, 0].max())\n",
    "plt.ylim(digits_pca[:, 1].min(), digits_pca[:, 1].max())\n",
    "for i in range(len(digits.data)):\n",
    "    # actually plot the digits as text instead of using scatter\n",
    "    plt.text(digits_pca[i, 0], digits_pca[i, 1], str(digits.target[i]),\n",
    "             color = colors[digits.target[i]],\n",
    "             fontdict={'weight': 'bold', 'size': 9})\n",
    "plt.xlabel(\"first principal component\")\n",
    "plt.ylabel(\"second principal component\");"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.manifold import TSNE\n",
    "tsne = TSNE(random_state=42)\n",
    "# use fit_transform instead of fit, as TSNE has no transform method:\n",
    "digits_tsne = tsne.fit_transform(digits.data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10, 10))\n",
    "plt.xlim(digits_tsne[:, 0].min(), digits_tsne[:, 0].max() + 1)\n",
    "plt.ylim(digits_tsne[:, 1].min(), digits_tsne[:, 1].max() + 1)\n",
    "for i in range(len(digits.data)):\n",
    "    # actually plot the digits as text instead of using scatter\n",
    "    plt.text(digits_tsne[i, 0], digits_tsne[i, 1], str(digits.target[i]),\n",
    "             color = colors[digits.target[i]],\n",
    "             fontdict={'weight': 'bold', 'size': 9})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Independent Component Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "May be a better choice than PCA for non-Gaussian distributions.\n",
    "\n",
    "From: http://scikit-learn.org/stable/auto_examples/decomposition/plot_ica_vs_pca.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import decomposition\n",
    "def plotTwoCol(data, ax, *args, **kwargs):\n",
    "    ax.plot(data[:,0], data[:,1], *args, **kwargs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using a `t` distribution with small degrees of freedom (non-Gaussian)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "src = np.random.standard_t(1.5, size=(1000,2))\n",
    "src[:, 0] *= 2\n",
    "\n",
    "# create observed data as mixture of src\n",
    "mix = np.array([[1, 1],   \n",
    "                [0, 2]])\n",
    "obs = np.dot(src, mix.T)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# obs[i] = <src[i]*A[0], src[i]*A[1]>\n",
    "print(np.dot(src[0], mix[0,:]), np.dot(src[0], mix[1,:]))\n",
    "print(src[0], \"-->\", obs[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "obs /= np.std(obs, axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, (ax1, ax2) = plt.subplots(1,2,figsize=(10,5), sharey=True)\n",
    "plotTwoCol(src/np.std(src,axis=0), ax1, \"b.\")\n",
    "plotTwoCol(obs, ax2, 'r.')\n",
    "\n",
    "ax1.set_title(\"True Sources\")\n",
    "ax2.set_title(\"Observed Data\")\n",
    "\n",
    "ax1.set_ylim(-4, 4)\n",
    "ax1.set_xlim(-4, 4)\n",
    "ax2.set_xlim(-4, 4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pca = decomposition.PCA()\n",
    "rec_pca = pca.fit_transform(obs) # .transform(X)\n",
    "\n",
    "ica = decomposition.FastICA()\n",
    "rec_ica = ica.fit_transform(obs) # .transform(X)  # Estimate the sources"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, (ax1, ax2) = plt.subplots(1,2,figsize=(10,5), sharey=True)\n",
    "plotTwoCol(rec_pca / np.std(rec_pca, axis=0), ax1, 'b.')\n",
    "plotTwoCol(rec_ica / np.std(rec_ica, axis=0), ax2, 'r.')\n",
    "\n",
    "ax1.set_title(\"Recovered via PCA\")\n",
    "ax2.set_title(\"Recovered via ICA\")\n",
    "\n",
    "ax1.set_ylim(-4,4)\n",
    "ax1.set_xlim(-4,4)\n",
    "ax2.set_xlim(-4,4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes =  plt.subplots(2, 2, figsize=(10,10), sharey=True, sharex=True)\n",
    "plotTwoCol(src/np.std(src,axis=0), axes[0,0], \"b.\")\n",
    "plotTwoCol(obs, axes[0,1], 'r.')\n",
    "\n",
    "axes[0,0].set_title(\"True Sources\")\n",
    "axes[0,1].set_title(\"Observed Data\")\n",
    "\n",
    "plotTwoCol(rec_pca / np.std(rec_pca, axis=0), axes[1,0], 'g.')\n",
    "plotTwoCol(rec_ica / np.std(rec_ica, axis=0), axes[1,1], 'y.')\n",
    "\n",
    "axes[1,0].set_title(\"Recovered via PCA\")\n",
    "axes[1,1].set_title(\"Recovered via ICA\")\n",
    "\n",
    "for ax in axes.flat:\n",
    "    ax.set_xlim(-4,4)\n",
    "    ax.set_ylim(-4,4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Summary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this notebook, we reviewed the following topics in preparation for more advanced topics:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " * [Dimensionality Reduction, Feature Extraction and Manifold Learning](#Dimensionality-Reduction,-Feature-Extraction-and-Manifold-Learning)\n",
    " * [Principal Component Analysis (PCA)](#Principal-Component-Analysis-%28PCA%29)\n",
    " * [Non-Negative Matrix Factorization (NMF)](#Non-Negative-Matrix-Factorization-%28NMF%29)\n",
    " * [Manifold learning with t-SNE](#Manifold-learning-with-t-SNE)\n",
    " * [Independent Component Analysis](#Independent-Component-Analysis)\n",
    " * [Exercises](#Exercises)\n",
    " * [Summary](#Summary)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a href='Unsupervised_Feature_Extraction_Exercises.ipynb' class='btn btn-primary btn-lg'>Exercises</a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src='img/copyright.png'>"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python",
   "pygments_lexer": "ipython3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
