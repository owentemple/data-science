{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src='img/logo.png'>\n",
    "<img src='img/title.png'>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Linear Models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Linear models are used to predict a continuous quantity such as a *real number* or a *floating point number*."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Table of Contents\n",
    "* [Linear Models](#Linear-Models)\n",
    "\t* [Linear regression](#Linear-regression)\n",
    "\t\t* [Coefficients](#Coefficients)\n",
    "\t\t* [Ridge regression (L2 penalty)](#Ridge-regression-%28L2-penalty%29)\n",
    "\t\t* [Lasso regression (L1 penalty)](#Lasso-regression-%28L1-penalty%29)\n",
    "\t* [Exercise](#Exercise)\n",
    "* [Classification](#Classification)\n",
    "\t* [Logistic regression](#Logistic-regression)\n",
    "\t\t* [Probabilities](#Probabilities)\n",
    "\t\t* [Decision boundaries](#Decision-boundaries)\n",
    "\t\t* [Regularization](#Regularization)\n",
    "\t* [Linear Support Vector Classifier](#Linear-Support-Vector-Classifier)\n",
    "\t\t* [Regularization](#Regularization)\n",
    "* [How to choose a linear model](#How-to-choose-a-linear-model)\n",
    "* [Pitfalls of linear models](#Pitfalls-of-linear-models)\n",
    "\t* [Exercise](#Exercise)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import src.mglearn as mglearn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Linear regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Linear regression methods assume that:\n",
    "* as a feature value varies the target value varies proportionally\n",
    "  * responses to features are global\n",
    "  * responses are strictly linear\n",
    "* features are not co-linear"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Minimizing the feature coefficients and intercept:\n",
    "\n",
    "$$ \\text{min}_{w, b} \\sum_i || w^\\mathsf{T}x_i + b  - y_i||^2 $$\n",
    "\n",
    "\n",
    "See the [full help on LinearRegression](http://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LinearRegression.html) in scikit-learn.\n",
    "\n",
    "Arguments include:\n",
    " * `fit_intercept`: fit an intercept to the model\n",
    " * `normalize`: whether to normalize the X data first, if fit_intercept is False data are assumed centered already"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "auto = pd.read_csv('data/auto-mpg.csv')\n",
    "auto.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X = auto[['cyl','displ','hp','weight','accel']]\n",
    "y = auto['mpg']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LinearRegression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y)\n",
    "lr = LinearRegression()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "lr.fit(X_train, y_train)\n",
    "lr.score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Coefficients"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class='alert alert-info'>\n",
    "<img src='img/topics/Essential-Concept.png' align='left' style='padding:10px'>\n",
    "<big><big><br>\n",
    "LinearRegression models store the coefficients for each *feature* in <tt>.coef\\_</tt> and <tt>.intercept\\_</tt>.\n",
    "<br><br>\n",
    "</big></big>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "lr.intercept_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "pd.Series(lr.coef_, index=X.columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ridge regression (L2 penalty)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ridge regression may be a good choice when there are colinear predictors in the `X` matrix.  This [intro to ridge regression](https://onlinecourses.science.psu.edu/stat857/node/155) discusses some of the motivation for choosing ridge regression over ordinary least squares. \n",
    "\n",
    "The second term in the right hand side of the equation below is the L2 regularization that is part of ridge regression.  Ridge regression supports multivariate regression (`Y` data of a shape `(n_samples, n_targets)` in addition to univariate regression shown here (1-D `Y` data).\n",
    "\n",
    "$$ \\text{min}_{w,b}  \\sum_i || w^\\mathsf{T}x_i + b  - y_i||^2  + \\alpha ||w||_2^2$$ \n",
    "\n",
    "$\\alpha$ is usually chosen between 0.01 and 100.\n",
    "\n",
    "The link above provides a geometric interpretation of ridge regression mutually minimizing the error terms and the penalty constant:\n",
    "\n",
    "![Geometric Interpretation of Ridge Regression](img/ridge_regression_geomteric.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Keep the parameters close to zero."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The [documentation](http://scikit-learn.org/stable/modules/generated/sklearn.linear_model.Ridge.html#sklearn.linear_model.Ridge.score) for `.score` of `Ridge` models shows `.score` is returning the __R<sup>2</sup>__ of the regression."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_boston\n",
    "boston = load_boston()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "houses = pd.DataFrame(data=boston.data, columns=boston.feature_names)\n",
    "houses['price'] = boston.target\n",
    "houses.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X = houses.drop('price', axis='columns')\n",
    "y = houses['price']\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.linear_model import Ridge\n",
    "lr = Ridge(alpha=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "lr.fit(X_train, y_train)\n",
    "lr.score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "pd.Series(lr.coef_, index=X.columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice that NOX and RM are far from zero."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "ridge = Ridge(alpha=10)\n",
    "\n",
    "ridge.fit(X_train, y_train)\n",
    "ridge.score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The largest parameters were damped."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "pd.Series(ridge.coef_, index=X.columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Lasso regression (L1 penalty)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**L**east **a**bsolute **s**hrinkage **s**election **o**perator.\n",
    "\n",
    "L1 penalty is seen in the final term on right hand side below:\n",
    "\n",
    "$$ \\text{min}_{w, b} \\sum_i || w^\\mathsf{T}x_i + b  - y_i||^2  + \\alpha ||w||_1$$ \n",
    "\n",
    " * If `X` data are highly colinear, choose an `L2` rather than `L1` regularization\n",
    " * Again [`.score` is returning](http://scikit-learn.org/stable/modules/generated/sklearn.linear_model.Lasso.html#sklearn.linear_model.Lasso.score) the __R<sup>2</sup>__ of the fit\n",
    " \n",
    "Visually contrasted with Ridge:\n",
    "\n",
    "![L1 versus L2 regions](img/L1_and_L2_balls.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is conceptually simpler than Ridge because coefficients will be set to zero."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.linear_model import Lasso"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "lasso = Lasso(0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "lasso.fit(X_train, y_train)\n",
    "lasso.score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice that NOX is now completely removed from the regression."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "pd.Series(lasso.coef_, index=X.columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load the bank campaign dataset from the last exercise.\n",
    "Try Ridge and Lasso linear models and plot the coefficients.\n",
    "Which features are important?\n",
    "Change the regularization parameter and check the changes in training set performance and test set performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# your solution here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Classification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Logistic regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "classification based on probabilites"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_iris\n",
    "iris = load_iris()\n",
    "\n",
    "print(iris['DESCR'][:471])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X = iris.data\n",
    "y = iris.target\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "logit = LogisticRegression()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "logit.fit(X_train, y_train)\n",
    "logit.score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Probabilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "logit.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "pd.DataFrame(logit.predict_proba(X_test), columns=iris.target_names).head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Decision boundaries"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For ease of plotting we'll just train the model over two features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "logit = LogisticRegression()\n",
    "logit.fit(X_train[:, 2:4], y_train)\n",
    "logit.score(X_test[:, 2:4], y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that the decision boundary between veriscolor and verginica is not well optimized."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(12,8))\n",
    "ax.scatter(X[:, 2], X[:, 3], c=y)\n",
    "mglearn.plot_2d_separator.plot_2d_classification(logit, X[:, 2:4], alpha=0.2, ax=ax)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Regularization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The $l1$ penalty behaves like Lasso regression and will force coefficients to zero.\n",
    "\n",
    "The $l2$ penalty behaves like Ridge regression and damps coefficients.\n",
    "\n",
    "For logistic regressions with many features this can help avoid overfitting.\n",
    "\n",
    "Even for these two features, regularization significantly improves the decision boundary between versicolor and virginica.\n",
    "\n",
    "Larger values of C mean less regularization:\n",
    "* large values of C attempt fit the training data as best as possible\n",
    "* small values of C attempt to keep the paramters close to zero\n",
    "\n",
    "Large values of C may lead to overfitting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "logit_l1 = LogisticRegression(penalty='l1', C=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "logit_l1.fit(X_train[:, 2:4], y_train)\n",
    "logit_l1.score(X_test[:, 2:4], y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(12,8))\n",
    "ax.scatter(X[:, 2], X[:, 3], c=y)\n",
    "mglearn.plot_2d_separator.plot_2d_classification(logit_l1, X[:, 2:4], alpha=0.2, ax=ax)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "logit_l2 = LogisticRegression(penalty='l2', C=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "logit_l2.fit(X_train[:, 2:4], y_train)\n",
    "logit_l2.score(X_test[:, 2:4], y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(12,8))\n",
    "ax.scatter(X[:, 2], X[:, 3], c=y)\n",
    "mglearn.plot_2d_separator.plot_2d_classification(logit_l2, X[:, 2:4], alpha=0.2, ax=ax)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Linear Support Vector Classifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Instead of looking for probabilities between two more classes the LinearSVC searches for a *hyperplane* through the features where classes can be separated."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.svm import LinearSVC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "svc = LinearSVC()\n",
    "svc.fit(X_train[:, 2:4], y_train)\n",
    "svc.score(X_test[:, 2:4], y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(12,8))\n",
    "ax.scatter(X[:, 2], X[:, 3], c=y)\n",
    "mglearn.plot_2d_separator.plot_2d_classification(svc, X[:, 2:4], alpha=0.2, ax=ax)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Regularization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The regularization paramter is `C` and behaves the same way as in LogisticRegression."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "svc = LinearSVC(C=100)\n",
    "svc.fit(X_train[:, 2:4], y_train)\n",
    "svc.score(X_test[:, 2:4], y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(12,8))\n",
    "ax.scatter(X[:, 2], X[:, 3], c=y)\n",
    "mglearn.plot_2d_separator.plot_2d_classification(svc, X[:, 2:4], alpha=0.2, ax=ax)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# How to choose a linear model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The most important choice in linear models is the penalty function.\n",
    "\n",
    "* $l1$ penalty (Lasso)\n",
    "    * When you assume that some features are unimportant and their coefficients go to zero.\n",
    "    * Easy to interpret.\n",
    "* $l2$ penalty (Ridge)\n",
    "    * Utilize all features and damp large coefficients\n",
    "    \n",
    "The second choice it how much regularization to apply\n",
    "\n",
    "* Weak regularization\n",
    "    * large C in $l2$; small alpha in $l1$\n",
    "    * the model more closely fits the training data\n",
    "    * a more complex model that may perform well in many dimensions \n",
    "* Strong regularization\n",
    "    * small C in $l2$; large alpha in $l1$\n",
    "    * may cure overfitting where training and testing scores are very different\n",
    "    * may decrease both training and testing scores; underfitting"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pitfalls of linear models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Famously, there can be many features of data that are not well captured in any (naïve) linear model. [Francis Anscombe](https://en.wikipedia.org/wiki/Frank_Anscombe) created his [Anscombe's quartet](https://en.wikipedia.org/wiki/Anscombe%27s_quartet) as an illustration of this. The several distributions in it have nearly the same or identical means and variances along X and Y axes, have almost the same correlations of X and Y, have the same linear regression lines and coefficient of determination.  Visually the distributions are obviously different, however.\n",
    "\n",
    "![Anscombe's quartet](img/Anscombe_quartet.png)\n",
    "\n",
    "Humorously, Justin Matejka and George Fitzmaurice give a similar example with the [Datasaurus](https://www.autodeskresearch.com/publications/samestats) in _Same Stats, Different Graphs: Generating Datasets with Varied Appearance and Identical Statistics through Simulated Annealing_:\n",
    "\n",
    "<img src=\"img/DataDino-600x455.gif\" width=\"50%\"/>\n",
    "\n",
    "Let us look at some basic statistics and regressions on this data. Let us also look at the statistical properties of the subsets defined in the dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "df = sns.load_dataset(\"anscombe\")\n",
    "df_1 = df[df.dataset=='I']\n",
    "df_2 = df[df.dataset=='II']\n",
    "df_3 = df[df.dataset=='III']\n",
    "df_4 = df[df.dataset=='IV']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us also look at the statistical properties of the subsets defined in the dataframe. In particular, notice the means and standard deviations (of both the x and y features), and the min/max and quartiles of the first three x feature collections."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "print(\"I\\n\",     df_1.describe())\n",
    "print(\"\\nII\\n\",  df_2.describe())\n",
    "print(\"\\nIII\\n\", df_3.describe())\n",
    "print(\"\\nIV\\n\",  df_4.describe())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The overall data collection has similar (but not quite identical) properties to its subsects."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "print(df.describe())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We might hope to tease apart what is going on with the several subsets by looking at the correlation of the features. Other than some floating point approximations, these are also identical."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "(np.corrcoef(df_1.x, df_1.y)[1,0],\n",
    " np.corrcoef(df_2.x, df_2.y)[1,0],\n",
    " np.corrcoef(df_3.x, df_3.y)[1,0],\n",
    " np.corrcoef(df_4.x, df_4.y)[1,0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Actually visualizing the data gives us a very distinctly different impression of the data subsets.  The linear regressions are identical (under an [ordinary least squares fit](https://en.wikipedia.org/wiki/Ordinary_least_squares) here, but the point would be the same with a different fitting regime)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "# Show the results of a linear regression within each dataset\n",
    "sns.lmplot(x=\"x\", y=\"y\", col=\"dataset\", hue=\"dataset\", data=df,\n",
    "           col_wrap=2, palette=\"muted\", size=4,\n",
    "           scatter_kws={\"s\": 50, \"alpha\": 1});"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Characterize the several subset distributions.  Think about techniques that could used to create more meaningful (linear) models of each of the distributions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src='img/copyright.png'>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
