{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src='img/logo.png'>\n",
    "<img src='img/title.png'>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Decision Trees and Random Forests"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* decision trees successively divide the problem into regions\n",
    "* random forests generally require little *feature engineering*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Table of Contents\n",
    "* [Decision Trees and Random Forests](#Decision-Trees-and-Random-Forests)\n",
    "* [Setup](#Setup)\n",
    "* [Classification Trees](#Classification-Trees)\n",
    "\t* [Important features](#Important-features)\n",
    "\t* [Visualization](#Visualization)\n",
    "* [Predict salaries](#Predict-salaries)\n",
    "* [Ensembles of trees](#Ensembles-of-trees)\n",
    "\t* [Random forest](#Random-forest)\n",
    "\t\t* [Feature importance](#Feature-importance)\n",
    "\t* [Gradient Boosting](#Gradient-Boosting)\n",
    "\t\t* [Important features](#Important-features)\n",
    "\t* [Ranking features](#Ranking-features)\n",
    "* [Regression problems](#Regression-problems)\n",
    "\t* [Complexity vs generality](#Complexity-vs-generality)\n",
    "\t* [Ensembles](#Ensembles)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import src.mglearn as mglearn\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Classification Trees"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Decision for classification problems allow for complex relationships to be modeled. Decision trees are called *white box* models since the decision process, while possibly lengthy, can always be interpreted.\n",
    "\n",
    "Further, decision trees evaluate features independently so they do not require normalization.\n",
    "\n",
    "A decision tree is built like the game *20 Questions*. The total number of questions is called `max_depth`.\n",
    "\n",
    "1. Choose a feature (A) and choose a dividing value to optimally separate the labels\n",
    "2. In each region of A divide along a second feature (B) the same way.\n",
    "3. Continue until\n",
    "   * each new region contains 1 observation or observations of the same class.\n",
    "   * or the number of questions has been exhausted.\n",
    "   \n",
    "The act of limiting the number of leaves is called *pruning*. Decision trees in scikit-learn can only be *pre-pruned*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "mglearn.plot_interactive_tree.plot_tree_progressive()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's try to use a decision tree to model survival from the Titanic."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "titanic = pd.read_csv('data/titanic.csv').dropna()\n",
    "titanic.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Like Naive Bayes models we need to encode categorical features as True/False."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "features = ['class', 'age', 'deck', 'survived']\n",
    "titanic_dummies = pd.get_dummies(titanic[features])\n",
    "titanic_dummies.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "train, test = train_test_split(titanic_dummies, random_state=0)\n",
    "\n",
    "features = titanic_dummies.columns.drop('survived')\n",
    "\n",
    "X_train = train[features]\n",
    "X_test = test[features]\n",
    "y_train = train['survived']\n",
    "y_test = test['survived']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The most important feature of a decision tree is the `max_depth`.\n",
    "\n",
    "A decision tree with a large `max_depth`, while capable of defining every observation, is by definition *overfitted.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tree = DecisionTreeClassifier(max_depth=3)\n",
    "\n",
    "tree.fit(X_train, y_train)\n",
    "tree.score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Important features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By asking only three independent questions we can see which three features have most *decisive power*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "pd.Series(tree.feature_importances_, index=features).plot.bar(figsize=(18,7))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.tree import export_graphviz\n",
    "import sys, subprocess\n",
    "from IPython.display import Image\n",
    "\n",
    "export_graphviz(tree, feature_names=features, class_names=['dead','alive'],\n",
    "                out_file='tmp/titanic.dot', impurity=False, filled=True)\n",
    "subprocess.check_call([sys.prefix+'/bin/dot','-Tpng','tmp/titanic.dot','-o','tmp/titanic.png'])\n",
    "Image('tmp/titanic.png')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Predict salaries"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Given a collection of demographic information. What are the most important features to predict whether someone makes more than $50,000 per year?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "salaries = pd.read_csv('data/adult.csv', index_col=0)\n",
    "salaries.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "salaries_dummies = pd.get_dummies(salaries).drop('income_ <=50K', axis='columns')\n",
    "features = salaries_dummies.columns.drop('income_ >50K')\n",
    "\n",
    "train, test = train_test_split(salaries_dummies, random_state=0)\n",
    "\n",
    "X_train = train[features]\n",
    "X_test = test[features]\n",
    "y_train = train['income_ >50K']\n",
    "y_test = test['income_ >50K']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's overfit a Decision Tree."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tree = DecisionTreeClassifier(max_depth=None, random_state=0)\n",
    "tree.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The model is 98% accurate on the *training data*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tree.score(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "But 81% accurate on the testing data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tree.score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "pd.Series(tree.feature_importances_, index=X_train.columns).plot.bar(figsize=(18,7))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ensembles of trees"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ensemble models help reduce the effects of overfitting a single decision tree by creating an ensemble of *slightly different* decision trees.\n",
    "\n",
    "* Trees can differ by\n",
    "  * the subset of training data used to train\n",
    "  * the subset of features used to train"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Random forest"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Random forests build independent decision trees and by default each tree is given a random subset of the training data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`n_estimators` is total number of trees to fit. Here we'll let all of the Decision Trees overfit.\n",
    "\n",
    "With this we get a test score slightly higher."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "rnd = RandomForestClassifier(n_estimators=5, random_state=0)\n",
    "\n",
    "rnd.fit(X_train, y_train)\n",
    "print('Training {}'.format(rnd.score(X_train, y_train)))\n",
    "print('Testing  {}'.format(rnd.score(X_test, y_test)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature importance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ensemble methods are generally more stable than decision trees and as such the feature importantces is highly reliable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "pd.Series(rnd.feature_importances_, index=X_train.columns).plot.bar(figsize=(18,7))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gradient Boosting"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Gradient Boosting trees are built sequentially by correcting mistakes from the previous tree.\n",
    "\n",
    "In Gradient Boosting each tree must be pre-pruned.\n",
    "\n",
    "Caution: Gradient Boosting models can be slow to train."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.ensemble import GradientBoostingClassifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Like trees and forests `max_depth` controls the number of decision layers in each tree.\n",
    "\n",
    "The `learning_rate` argument controls the strength of the corrects to each sequential tree.\n",
    "* higher `learning_rate` can lead to a complex model and overfitting \n",
    "* lower `learning_rate` can lead to a general model and underfitting\n",
    "\n",
    "The `n_estimators` parameter defaults to 100 trees."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "gb = GradientBoostingClassifier(max_depth=2, learning_rate=0.01, n_estimators=100, random_state=0)\n",
    "\n",
    "gb.fit(X_train, y_train)\n",
    "print('Training {}'.format(gb.score(X_train, y_train)))\n",
    "print('Testing  {}'.format(gb.score(X_test, y_test)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By changing max_depth, learning_rate and n_estimators we can improve the scores.\n",
    "\n",
    "Caution: Gradient Boosting models can be sensitive to small changes in these parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "gb = GradientBoostingClassifier(max_depth=3, learning_rate=0.1, n_estimators=100, random_state=0)\n",
    "\n",
    "gb.fit(X_train, y_train)\n",
    "print('Training {}'.format(gb.score(X_train, y_train)))\n",
    "print('Testing  {}'.format(gb.score(X_test, y_test)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Important features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "pd.Series(gb.feature_importances_, index=X_train.columns).plot.bar(figsize=(18,7))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ranking features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The forest of simple trees helps reduce overfitting, but the decisions are less easy to understand since the decision boundaries are determined by *soft voting* where regions overlap."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "pd.Series(rnd.feature_importances_, index=X_train.columns).plot.bar(figsize=(18,7))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Regression problems"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Decision trees are built by making divisions along a feature \n",
    "\n",
    "The dividing lines are optimized to reduce prediction errors to the training values.\n",
    "\n",
    "These separate dividing lines allow for *non-linear* responses to be trained."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "auto = pd.read_csv('data/auto-mpg.csv')\n",
    "auto.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X = auto[['cyl','displ','hp','weight','accel']]\n",
    "y = auto['mpg']\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Caution**\n",
    "\n",
    "Decision tree regressors cannot *extrapolate* data. They are best used to predict new values within the range of training features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.tree import DecisionTreeRegressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tree = DecisionTreeRegressor(random_state=0, max_depth=2)\n",
    "\n",
    "features = ['hp']\n",
    "\n",
    "tree.fit(X_train[features], y_train)\n",
    "tree.score(X_test[features], y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The terminal leaves in the graph show the target value associated with each decision path.\n",
    "\n",
    "The predicted value of the region for each terminal leaf is the mean value of the training target."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "export_graphviz(tree, feature_names=features,\n",
    "                out_file='tmp/auto.dot', impurity=False, filled=True)\n",
    "subprocess.check_call([sys.prefix+'/bin/dot','-Tpng','tmp/auto.dot','-o','tmp/auto.png'])\n",
    "Image('tmp/auto.png')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Complexity vs generality"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As the max_depth increases the model can easily *memorize* the training data and become incapable of making accurate predictions over the test data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(ncols=3, figsize=(20,8))\n",
    "\n",
    "for ax, depth in zip(axes, [2, 4, 8]):\n",
    "    tree = DecisionTreeRegressor(random_state=0, max_depth=depth)\n",
    "    \n",
    "    tree.fit(X_train[['hp']], y_train)\n",
    "    tree.score(X_test[['hp']], y_test)\n",
    "    \n",
    "    auto.plot.scatter(x='hp', y='mpg', ax=ax)\n",
    "    s = X.sort_values('hp')\n",
    "    ax.plot(s['hp'], tree.predict(s[['hp']]), c='red')\n",
    "    ax.set_title('max_depth={}'.format(depth))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ensembles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_boston\n",
    "boston = load_boston()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "houses = pd.DataFrame(boston.data, columns=boston.feature_names)\n",
    "houses['price'] = boston.target\n",
    "\n",
    "X = houses.drop('price',axis='columns')\n",
    "y = houses['price']\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When training on multiple features a division is made in only one dimension at a time.\n",
    "\n",
    "This can be used to aid in *feature selection*.\n",
    "\n",
    "This model is intentionally overfit."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tree = DecisionTreeRegressor(random_state=0)\n",
    "tree.fit(X_train, y_train)\n",
    "tree.score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The relative *importance* of a feature is a measure of its predictive power.\n",
    "\n",
    "**Caution**\n",
    "\n",
    "In the case of co-linear features only one will be usually have a high important since adding a decision node in the second feature makes little difference."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "pd.Series(tree.feature_importances_, index=boston.feature_names).plot.bar(figsize=(18,7))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Random Forest improves the accuracy without having to aggressively tune parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestRegressor\n",
    "\n",
    "rnd = RandomForestRegressor(n_estimators=10, random_state=0)\n",
    "rnd.fit(X_train, y_train)\n",
    "rnd.score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "pd.Series(rnd.feature_importances_, index=boston.feature_names).plot.bar(figsize=(18,7))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And finally Gradient Boosting has improved the score a bit further."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.ensemble import GradientBoostingRegressor\n",
    "\n",
    "gbr = GradientBoostingRegressor(n_estimators=100, max_depth=5, learning_rate=0.1, random_state=0)\n",
    "gbr.fit(X_train, y_train)\n",
    "gbr.score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "pd.Series(gbr.feature_importances_, index=boston.feature_names).plot.bar(figsize=(18,7))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src='img/copyright.png'>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
