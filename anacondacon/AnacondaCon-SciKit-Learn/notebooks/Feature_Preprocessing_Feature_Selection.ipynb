{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src='img/logo.png'>\n",
    "<img src='img/title.png'>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Table of Contents\n",
    "* [Feature extraction and selection](#Feature-extraction-and-selection)\n",
    "\t* [Categorical Variables](#Categorical-Variables)\n",
    "\t\t* [One-Hot-Encoding (Dummy variables)](#One-Hot-Encoding-%28Dummy-variables%29)\n",
    "\t\t\t* [Checking string-encoded categorical data](#Checking-string-encoded-categorical-data)\n",
    "\t\t* [Numbers can encode categoricals](#Numbers-can-encode-categoricals)\n",
    "\t* [Binning, Discretization, Linear Models and Trees](#Binning,-Discretization,-Linear-Models-and-Trees)\n",
    "\t* [Interactions and Polynomials](#Interactions-and-Polynomials)\n",
    "\t\t* [Scaling before adding polynomial terms](#Scaling-before-adding-polynomial-terms)\n",
    "\t* [Univariate Non-linear transformations](#Univariate-Non-linear-transformations)\n",
    "\t* [Automatic Feature Selection](#Automatic-Feature-Selection)\n",
    "\t\t* [Univariate statistics](#Univariate-statistics)\n",
    "\t\t* [Model-based Feature Selection](#Model-based-Feature-Selection)\n",
    "\t\t* [Recursive Feature Elimination](#Recursive-Feature-Elimination)\n",
    "* [Summary](#Summary)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature extraction and selection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Feature extraction can include encoding of text and categorical data to a sparse integer matrix, as shown in this notebook.\n",
    "\n",
    "Feature extraction can also include more specialized text processing, such as word counters, term frequency - inverse document frequency transformation, or hashing vectorization, which are all discussed in the Scikit-learn documentation page [Working with Text Data](http://scikit-learn.org/stable/tutorial/text_analytics/working_with_text_data.html)\n",
    "\n",
    "More documentation of extraction:\n",
    "\n",
    "http://scikit-learn.org/stable/auto_examples/cluster/plot_dict_face_patches.html\n",
    "\n",
    "http://scikit-learn.org/stable/modules/feature_extraction.html#patch-extraction\n",
    "\n",
    "Feature selection consists of using feature statistics to find a subset of the predictor matrix columns that is likely to explain variance in the dependent data set.  Here is an overview of feature selection with scikit-learn:\n",
    "\n",
    "http://scikit-learn.org/stable/modules/feature_selection.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hide_input": false
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "\n",
    "plt.rcParams['image.interpolation'] = \"none\"\n",
    "np.set_printoptions(precision=3)\n",
    "\n",
    "import src.mglearn as mglearn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Categorical Variables"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### One-Hot-Encoding (Dummy variables)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Expansion of categorical string columns to dummy numeric matrices."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "data = pd.read_csv(os.path.join(\"data\", \"adult.csv\"))\n",
    "data = data[['age', 'workclass', 'education', 'gender', 'hours-per-week', 'occupation', 'income']]\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Checking string-encoded categorical data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.gender.value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note the expansion of the matrix columns with `OneHotEncoder`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Original features:\\n\", list(data.columns), \"\\n\")\n",
    "data_dummies = pd.get_dummies(data)\n",
    "print(\"Features after get_dummies:\\n\", list(data_dummies.columns))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dummies.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get only the columns containing features, that is all columns from 'age' to 'occupation_ Transport-moving'\n",
    "# This range contains all the features but not the target\n",
    "\n",
    "features = data_dummies.ix[:, 'age':'occupation_ Transport-moving']\n",
    "# extract numpy arrays\n",
    "X = features.values\n",
    "y = data_dummies['income_ >50K'].values\n",
    "print(X.shape, y.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With the categorical string columns now expanded into new indicator integer columns, we can use the matrix as the `X` argument to a model method like `fit`.  Below we are passing the encoded categorical matrix to `LogistricRegression`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=0)\n",
    "logreg = LogisticRegression()\n",
    "logreg.fit(X_train, y_train)\n",
    "print(logreg.score(X_test, y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Numbers can encode categoricals"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following cells show the expansion of a dataframe originally containing a categorial column with 3 distinct values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a dataframe with an integer feature and a categorical string feature\n",
    "demo_df = pd.DataFrame({'Integer Feature': [0, 1, 2, 1], 'Categorical Feature': ['socks', 'fox', 'socks', 'box']})\n",
    "demo_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.get_dummies(demo_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "demo_df['Integer Feature'] = demo_df['Integer Feature'].astype(str)\n",
    "pd.get_dummies(demo_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Binning, Discretization, Linear Models and Trees"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Binning converts a continuous feature to a categorical one and can be useful as a preprocessing step before training / prediction and in statistical analysis more generally.  \n",
    "\n",
    "The following cells show how a `DecisionTreeRegressor` and `LinearRegressor` can give similar results with binning of input data.  See also [the help for `numpy.digitize`](http://docs.scipy.org/doc/numpy/reference/generated/numpy.digitize.html)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "\n",
    "X, y = mglearn.datasets.make_wave(n_samples=100)\n",
    "plt.plot(X[:, 0], y, 'o')\n",
    "line = np.linspace(-3, 3, 1000)[:-1].reshape(-1, 1);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lin_reg = LinearRegression().fit(X, y)\n",
    "dec_reg = DecisionTreeRegressor(min_samples_split=3).fit(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(X[:, 0], y, 'o')\n",
    "plt.plot(line, lin_reg.predict(line), label=\"linear regression\")\n",
    "plt.plot(line, dec_reg.predict(line), label=\"decision tree\")\n",
    "plt.ylabel(\"regression output\")\n",
    "plt.xlabel(\"input feature\")\n",
    "plt.legend(loc=\"best\");"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The figure above shows the effect of the `min_samples_split` on `DecisionTreeRegressor`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.set_printoptions(precision=2)\n",
    "bins = np.linspace(-3, 3, 11)\n",
    "bins"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "which_bin = np.digitize(X, bins=bins)\n",
    "print(\"\\nData points:\\n\", X[:5])\n",
    "print(\"\\nBin membership for data points:\\n\", which_bin[:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import OneHotEncoder\n",
    "# transform using the OneHotEncoder.\n",
    "encoder = OneHotEncoder(sparse=False)\n",
    "# encoder.fit finds the unique values that appear in which_bin\n",
    "encoder.fit(which_bin)\n",
    "# transform creates the one-hot encoding\n",
    "X_binned = encoder.transform(which_bin)\n",
    "print(X_binned[:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_binned.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "line_binned = encoder.transform(np.digitize(line, bins=bins))\n",
    "\n",
    "plt.plot(X[:, 0], y, 'o')\n",
    "reg = LinearRegression().fit(X_binned, y)\n",
    "plt.plot(line, reg.predict(line_binned), label='linear regression binned')\n",
    "\n",
    "reg = DecisionTreeRegressor(min_samples_split=3).fit(X_binned, y)\n",
    "plt.plot(line, reg.predict(line_binned), label='decision tree binned')\n",
    "for bin in bins:\n",
    "    plt.plot([bin, bin], [-3, 3], ':', c='k')\n",
    "plt.legend(loc=\"best\")\n",
    "plt.suptitle(\"linear_binning\");"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Interactions and Polynomials"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Interaction terms can form new columns in an input matrix when machine learning models should consider the correlation rather than only the independent effects of features. \n",
    "\n",
    "The next few cells demonstrate the effect of an interaction term in linear regressions on binned time series."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_combined = np.hstack([X, X_binned])\n",
    "print(X_combined.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(X[:, 0], y, 'o')\n",
    "\n",
    "reg = LinearRegression().fit(X_combined, y)\n",
    "\n",
    "line_combined = np.hstack([line, line_binned])\n",
    "plt.plot(line, reg.predict(line_combined), label='linear regression combined')\n",
    "\n",
    "for bin in bins:\n",
    "    plt.plot([bin, bin], [-3, 3], ':', c='k')\n",
    "plt.legend(loc=\"best\");"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_product = np.hstack([X_binned, X * X_binned])\n",
    "print(X_product.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(X[:, 0], y, 'o')\n",
    "    \n",
    "reg = LinearRegression().fit(X_product, y)\n",
    "\n",
    "line_product = np.hstack([line_binned, line * line_binned])\n",
    "plt.plot(line, reg.predict(line_product), label='linear regression combined')\n",
    "\n",
    "for bin in bins:\n",
    "    plt.plot([bin, bin], [-3, 3], ':', c='k')\n",
    "plt.legend(loc=\"best\");"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Above we created the interaction term ourselves using `numpy` vectorized math.  `sklearn.preprocessing.PolynomialFeatures` does this automatically and [provides more options](http://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.PolynomialFeatures.html)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "\n",
    "# include polynomials up to x ** 10:\n",
    "poly = PolynomialFeatures(degree=10)\n",
    "poly.fit(X)\n",
    "X_poly = poly.transform(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_poly.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "poly.get_feature_names()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(X[:, 0], y, 'o')\n",
    "    \n",
    "reg = LinearRegression().fit(X_poly, y)\n",
    "\n",
    "line_poly = poly.transform(line)    # using the Poly transform\n",
    "plt.plot(line, reg.predict(line_poly), label='polynomial linear regression')\n",
    "plt.legend(loc=\"best\");"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Comparison of polynomial linear regression with a support vector regressor initialized under different `gamma` values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from sklearn.svm import SVR\n",
    "plt.plot(X[:, 0], y, 'o')\n",
    "\n",
    "for gamma in [1, 10]:\n",
    "    svr = SVR(gamma=gamma).fit(X, y)  # radial basis function kernel by default\n",
    "    plt.plot(line, svr.predict(line), label='SVR gamma=%d' % gamma)\n",
    "    \n",
    "plt.legend(loc=\"best\");"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Scaling before adding polynomial terms"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you plan to use scaling to normalize features and also add interaction or polynomial terms, it is generally best to do the scaling before adding features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_boston\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "boston = load_boston()\n",
    "X_train, X_test, y_train, y_test = train_test_split(boston.data, boston.target, random_state=0)\n",
    "\n",
    "# rescale data:\n",
    "scaler = MinMaxScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "poly = PolynomialFeatures(degree=2).fit(X_train_scaled)\n",
    "X_train_poly = poly.transform(X_train_scaled)\n",
    "X_test_poly = poly.transform(X_test_scaled)\n",
    "print(X_train.shape)\n",
    "print(X_train_poly.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(poly.get_feature_names())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import Ridge\n",
    "ridge = Ridge().fit(X_train_scaled, y_train)\n",
    "print(\"score without interactions: %f\" % ridge.score(X_test_scaled, y_test))\n",
    "ridge = Ridge().fit(X_train_poly, y_train)\n",
    "print(\"score with interactions: %f\" % ridge.score(X_test_poly, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestRegressor\n",
    "rf = RandomForestRegressor(n_estimators=100).fit(X_train_scaled, y_train)\n",
    "print(\"score without interactions: %f\" % rf.score(X_test_scaled, y_test))\n",
    "rf = RandomForestRegressor(n_estimators=100).fit(X_train_poly, y_train)\n",
    "print(\"score with interactions: %f\" % rf.score(X_test_poly, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rf.apply(X_test_poly)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rf.apply(X_test_poly).shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Univariate Non-linear transformations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A common univariate transform on continuous data is to log transform for data that are skewed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rnd = np.random.RandomState(0)\n",
    "X_org = rnd.normal(size=(1000, 3))\n",
    "w = rnd.normal(size=3)\n",
    "\n",
    "X = np.random.poisson(10 * np.exp(X_org))\n",
    "y = np.dot(X_org, w)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.bincount(X[:, 0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bins = np.bincount(X[:, 0])\n",
    "plt.bar(range(len(bins)), bins, color='b')\n",
    "plt.ylabel(\"number of appearances\")\n",
    "plt.xlabel(\"value\");"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import Ridge\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=0)\n",
    "Ridge().fit(X_train, y_train).score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_log = np.log(X_train + 1)\n",
    "X_test_log = np.log(X_test + 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.hist(np.log(X_train_log[:, 0] + 1), bins=25, color='b');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Ridge().fit(X_train_log, y_train).score(X_test_log, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Automatic Feature Selection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Feature selection reduces the dimensionality of the input data.  Dimensionality reduction can:\n",
    " * Making the model more efficient computationally,\n",
    " * Improve fit of the model by removing redundant or low variance columns\n",
    " * Make the model easier to understand and present with fewer input data requirements"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Univariate statistics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This example uses [SelectPercentile](http://scikit-learn.org/stable/modules/generated/sklearn.feature_selection.SelectPercentile.html) to subset half the features.  `SelectPercentile` can taking a scoring function, defaulting to [`f_classif`, the ANOVA F-score](http://scikit-learn.org/stable/modules/generated/sklearn.feature_selection.f_classif.html)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_breast_cancer\n",
    "from sklearn.feature_selection import SelectPercentile\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "cancer = load_breast_cancer()\n",
    "\n",
    "# get deterministic random numbers\n",
    "rng = np.random.RandomState(42)\n",
    "noise = rng.normal(size=(len(cancer.data), 50))\n",
    "# add noise features to the data\n",
    "# the first 30 features are from the dataset, the next 50 are noise\n",
    "X_w_noise = np.hstack([cancer.data, noise])\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X_w_noise, cancer.target, random_state=0, test_size=.5)\n",
    "# use f_classif (the default) and SelectPercentile to select 10% of features:\n",
    "select = SelectPercentile(percentile=50)\n",
    "select.fit(X_train, y_train)\n",
    "# transform training set:\n",
    "X_train_selected = select.transform(X_train)\n",
    "\n",
    "print(X_train.shape)\n",
    "print(X_train_selected.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_selection import f_classif, f_regression, chi2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "F, p = f_classif(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure()\n",
    "plt.plot(p, 'o');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mask = select.get_support()\n",
    "print(mask)\n",
    "# visualize the mask. black is True, white is False\n",
    "plt.matshow(mask.reshape(1, -1), cmap='gray_r');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "# transform test data:\n",
    "X_test_selected = select.transform(X_test)\n",
    "\n",
    "lr = LogisticRegression()\n",
    "lr.fit(X_train, y_train)\n",
    "print(\"Score with all features: %f\" % lr.score(X_test, y_test))\n",
    "lr.fit(X_train_selected, y_train)\n",
    "print(\"Score with only selected features: %f\" % lr.score(X_test_selected, y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model-based Feature Selection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use a model to do feature selection.  From [the docs](http://scikit-learn.org/stable/modules/generated/sklearn.feature_selection.SelectFromModel.html):\n",
    "\n",
    "`sklearn.feature_selection.SelectFromModel(estimator, threshold=None, prefit=False)`\n",
    "\n",
    "Parameters:\n",
    "   * `estimator`: An `sklearn` model\n",
    "   * `threshold`: Threshold of importance determining whether to keep a feature.  Examples: `'median'`, `'mean'`, `'1.25*mean'`\n",
    "   * `prefit`: `False` by default - set it to `True` if the `estimator` has already been `fit`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_selection import SelectFromModel\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "select = SelectFromModel(RandomForestClassifier(n_estimators=100, random_state=42),\n",
    "                         threshold=\"median\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now when we call `transform` we will get a matrix with the half the features (columns) of the original (`threshold=\"median\"`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "select.fit(X_train, y_train)\n",
    "X_train_l1 = select.transform(X_train)\n",
    "print(X_train.shape)\n",
    "print(X_train_l1.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mask = select.get_support() # indices of the columns that were selected\n",
    "# visualize the mask. black is True, white is False\n",
    "plt.matshow(mask.reshape(1, -1), cmap='gray_r');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test_l1 = select.transform(X_test)\n",
    "LogisticRegression().fit(X_train_l1, y_train).score(X_test_l1, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Recursive Feature Elimination"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Recursive feature elimination tries different sets of features until finding the smallest explantory set of features.  Read [more here]().\n",
    "\n",
    "`sklearn.feature_selection.RFE(estimator, n_features_to_select=None, step=1, verbose=0)`\n",
    "\n",
    "`step=1` in the arguments is the number of features to try dropping on each iteration."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_selection import RFE\n",
    "select = RFE(RandomForestClassifier(n_estimators=100, random_state=42), n_features_to_select=40)\n",
    "#select = RFE(LogisticRegression(penalty=\"l1\"), n_features_to_select=40)\n",
    "\n",
    "select.fit(X_train, y_train)\n",
    "# visualize the selected features:\n",
    "mask = select.get_support()\n",
    "plt.matshow(mask.reshape(1, -1), cmap='gray_r');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_rfe = select.transform(X_train)\n",
    "X_test_rfe = select.transform(X_test)\n",
    "\n",
    "LogisticRegression().fit(X_train_rfe, y_train).score(X_test_rfe, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "select.score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Summary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this notebook, we reviewed the following topics in preparation for more advanced topics:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " * [Feature extraction and selection](#Feature-extraction-and-selection)\n",
    " * [Categorical Variables](#Categorical-Variables)\n",
    " * [Binning, Discretization, Linear Models and Trees](#Binning,-Discretization,-Linear-Models-and-Trees)\n",
    " * [Interactions and Polynomials](#Interactions-and-Polynomials)\n",
    " * [Scaling before adding polynomial terms](#Scaling-before-adding-polynomial-terms)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a href='Feature_Preprocessing_Feature_Selection_Exercises.ipynb' class='btn btn-primary btn-lg'>Exercises</a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src='img/copyright.png'>"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python",
   "pygments_lexer": "ipython3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
